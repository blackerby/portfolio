<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Welcome on Blackerby Portfolio</title><link>https://blackerby.github.io/portfolio/</link><description>Recent content in Welcome on Blackerby Portfolio</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://blackerby.github.io/portfolio/index.xml" rel="self" type="application/rss+xml"/><item><title>1. A High Level Metadata Workflow Overview</title><link>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-22-llc-workflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-22-llc-workflow/</guid><description>A High Level Metadata Workflow Overview # The semester is winding down and so is my time as a remote metadata intern for the Law Library of Congress. I ended up learning a ton about Unix text processing workhorses like aspell, awk, diff, find, grep, sed, and GNU Emacs as well as newer tools like Miller and Datasette. I incorporated these tools into several shell scripts to expedite my workflow, which is built around a python script I wrote to extract specific text from the PDF I was working with using regular expressions.</description></item><item><title>2. prep.sh and mlr</title><link>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-23-prep/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-23-prep/</guid><description>prep.sh and mlr # I am an inexperienced shell scripter, but this project provided opportunities to do some productive work with the shell. My workflow starts with running prep.sh, a script that takes 2 or 3 arguments and is explained below.
Shebang # I&amp;rsquo;m on macOS, so my shebang line references zsh.
#! /usr/bin/env zsh Arguments # FILE=$(realpath $1) # full path to file name. must be pdf START=$2 # starting page of pdf file if [[ -n $3 ]] then FINISH=$3 # ending page of pdf file fi Here, I use realpath to take an argument like .</description></item><item><title>3. get_summaries.py</title><link>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-24-get-summaries/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-24-get-summaries/</guid><description>get_summaries.py # I hope to convert this file to ipynb format so interested folks can try the code out on their own. The code is currently published on GitHub.
Environment Setup # The Shebang Line # We add a shebang line in case we want to make this file executable.
#!/usr/bin/env python Necessary Libraries # Now we import the libraries we need.
import argparse import sys import re import csv from pathlib import Path from more_itertools import chunked from PyPDF2 import PdfReader The role of each library is explained below:</description></item><item><title>4. Emacs Workflow</title><link>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-25-emacs-workflow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-25-emacs-workflow/</guid><description>Emacs Workflow # We&amp;rsquo;ve run prep.sh (and with it, get_summaries.py), so now we&amp;rsquo;ve got a directory full of text files to review. This is where GNU Emacs comes in.
Initially, I was doing all of this work on the command line, using iTerm2 on my Mac. I had a script that ran aspell on each text file before opening Neovim for each file for manual review. It worked fine, but it wasn&amp;rsquo;t very flexible, and to be honest, I&amp;rsquo;ve always wanted to become proficient in Emacs.</description></item><item><title>5. cleanup.sh</title><link>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-25-cleanup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-11-25-cleanup/</guid><description>cleanup.sh # With the text files edited, it&amp;rsquo;s time to clean up a little bit. That&amp;rsquo;s what cleanup.sh is for.
As usual, we start with the shebang line.
#!/usr/bin/env zsh We remove all of the no longer necessary backups. The -f makes sure there are no complaints in case no backups exist.
# remove temporary files rm -f *.bak rm -f *~ This section isn&amp;rsquo;t really necessary, but I added in to make working with one-off awk commands a little easier (tabs as delimiters are much easier to deal with than commas).</description></item><item><title>6. (meta)data(sette)</title><link>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-12-04-meta-data-sette/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/technology/metadata/2022-12-04-meta-data-sette/</guid><description>(meta)data(sette) # Intro # After running cleanup.sh I like to make a local copy of it as a tab-separated file I can do various things with, including storing it in SQLite database with Datasette which, among other things, allows me to link the metadata with their associated text files.
The Code # As usual, it&amp;rsquo;s a shell script.
#!/usr/bin/env zsh I use wget to save the data to my local machine.</description></item><item><title>Philosophy</title><link>https://blackerby.github.io/portfolio/docs/philosophy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/philosophy/</guid><description>Information, Technology, Humanity, and Purpose # ðŸ‘‰ This was originally submitted as my final essay for LS 500: Information Science and Technology. It has been lightly edited for publication on the web.
ðŸ‘‰ You can download the original version here I distinctly remember what first piqued my interest in computing and technology. I was in sixth or seventh grade, spending the night at a friend&amp;rsquo;s house, and before settling in for a late night and early morning of action movie after action movie followed by far too little sleep, we sat at the desktop computer in the basement guestroom.</description></item><item><title>Resume</title><link>https://blackerby.github.io/portfolio/docs/resume/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/resume/</guid><description>William T. Blackerby # wmblackerby@gmail.com â€¢ LinkedIn â€¢ Github â€¢ Blog
Education # The University of Alabama, Tuscaloosa, AL
Master of Library and Information Studies (online)
expected completion Spring 2023
Completed coursework: LS 500: Information Science and Technology, LS 501: Information and Communities, LS 535: Records Management, LS 566: Metadata and Semantic Web Fundamentals, LS 590: Linked Data, LS 506: Modern Cataloging and Classification, LS 562: Digital Libraries, LS 564: Programming for Digital Libraries, LS 570: Internship (Law Library of Congress Remote Metadata Internship) Coursework in progress Spring 2023: LS 513: Professional Paths, LS 569: Information Management, LS 570: Internship (Southern Music Research Center) The University of the South, Sewanee, TN</description></item><item><title>Visualizing Topics of 74th Congress Senate Bills and Joint Resolutions</title><link>https://blackerby.github.io/portfolio/docs/technology/topic-modeling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://blackerby.github.io/portfolio/docs/technology/topic-modeling/</guid><description>Visualizing Topics of 74th Congress Senate Bills and Joint Resolutions # ðŸ‘‰ This code is available to experiment with on Binder in a notebook with the filename tmr.ipynb. Be sure to check out the ner_mapping.ipynb notebook as well, in which I create a heatmap of states mentioned in Senate bills and joint resolutions during the 74th Congress.
In LS 562: Digital Libraries, we studied the application of machine learning tools to text analysis problems.</description></item></channel></rss>