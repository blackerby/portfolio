[{"id":0,"href":"/portfolio/docs/technology/metadata/2022-11-22-llc-workflow/","title":"1. A High Level Metadata Workflow Overview","section":"Metadata Workflow Series","content":" A High Level Metadata Workflow Overview # The semester is winding down and so is my time as a remote metadata intern for the Law Library of Congress. I ended up learning a ton about Unix text processing workhorses like aspell, awk, diff, find, grep, sed, and GNU Emacs as well as newer tools like Miller and Datasette. I incorporated these tools into several shell scripts to expedite my workflow, which is built around a python script I wrote to extract specific text from the PDF I was working with using regular expressions. I also did a fair amount of work with Google Sheets and SQLite, and towards the end of the internship I even dabbled with the Google Drive API.\nMy plan now is to write a series of posts detailing the steps in this workflow and my use of the tools mentioned above. Below is a high-level overview of my workflow as it stands towards the end of the semester.\nExtract text with the prep.sh script. Paste initial metadata into Google Sheets. This initial metadata comes from running the command mlr --c2t --headerless-csv-output cat 74_2_{PAGE_NUM}.csv | pbcopy. 74_2_{PAGE_NUM}.csv is one of the files generated by the the prep.sh script. Review each of the text files generated by prep.sh (this includes running aspell). Run cleanup.sh to delete backup files, move the working directory to its final location, and copy the names of the text files to the clipboard for pasting into Google Sheets. Run update_tsv_data.sh to create local copies of the metadata, load the metadata and text files into a SQLite database, and publish that database to Heroku. Stay tuned for more posts about each of the above steps, starting with the prep.sh script.\n"},{"id":1,"href":"/portfolio/docs/technology/metadata/2022-11-23-prep/","title":"2. prep.sh and mlr","section":"Metadata Workflow Series","content":" prep.sh and mlr # I am an inexperienced shell scripter, but this project provided opportunities to do some productive work with the shell. My workflow starts with running prep.sh, a script that takes 2 or 3 arguments and is explained below.\nShebang # I\u0026rsquo;m on macOS, so my shebang line references zsh.\n#! /usr/bin/env zsh Arguments # FILE=$(realpath $1) # full path to file name. must be pdf START=$2 # starting page of pdf file if [[ -n $3 ]] then FINISH=$3 # ending page of pdf file fi Here, I use realpath to take an argument like ../../74.2.pdf and expand it to its full path. The first argument, $1, is should be a path to a PDF file. $2 should be the page the python script begins processing from, and $3 should be the last page it processes.\nProcessing # # pull metadata into csv and write summaries out to text files # named following this schema: {congress}_{session}_{bill type}_{bill number}.txt if [[ -n $FINISH ]] then python3 get_summaries.py $FILE $START $FINISH else python3 get_summaries.py $FILE $START fi If there is no $3, no worries: we just pass the file path and the start page to the python script.\n# get file\u0026#39;s name for name of new directory and iteration purposes FILE_BASENAME=$(basename $FILE .pdf) Here, I use basename with the name of the file and the extension .pdf to get the basename of the file the script is processing. This will be used to name the directories created by the script.\n# make new directory and move generated files into it if [[ -n $FINISH ]] then NEW_DIR=\u0026#34;${FILE_BASENAME}_${START}_${FINISH}\u0026#34; else NEW_DIR=\u0026#34;${FILE_BASENAME}_${START}\u0026#34; fi Above, we name the directory that the files generated by the script will be moved into.\nManual processing of the bill summaries pointed out some common OCR errors, so below I run a sed script to deal with some of those.\nfor f in *.{txt,csv} do gsed -f common_errors.sed -i $f done Here\u0026rsquo;s what that script looks like:\ns/\\bJudiciarv\\b/Judiciary/ig s/\\bTvdings\\b/Tydings/g s/\\bbv\\b/by/g s/\\banv\\b/any/g s/\\bdenved\\b/derived/g s/\\bm\\b/in/g s/\\bpiopose\\b/propose/g s/\\bwhioh\\b/which/g s/\\beto.\\b/etc./g s/\\bwit inn\\b/within/g s/\\bwrorks\\b/works/g s/\\bLnited\\b/United/g s/\\bsolelv\\b/solely/g s/\\blavs\\b/lays/g s/\\bArmv\\b/Army/g s/\\biniured\\b/injured/g s/\\bDedares\\b/Declares/ig s/\\bcrcp\\b/crop/ig s/\\bWilev\\b/Wiley/ig s/\\bdomestio\\b/domestic/ig Simple stuff, but it saves some time.\nThe OCR preserved the typesetting of the lines in the original document, so I run another sed script to make the lines flow and to replace some of the entities the script brings in and normalize some of the punctuation.\nfor f in *.txt do gsed -i.bak -Ez -f clean_lines.sed $f done Here\u0026rsquo;s that script:\ns/- \\n//g s/(\\.:) \\n/\\1\\n\\n/g s/([^.:] )\\n/\\1/g s/(‘‘|’’|“|”)/\u0026#34;/g s/[‘’]/\u0026#39;/g s/-?— ?/--/g s/■//g s/ (#|\\.+) / /g s/(\\w\\.)\\.+/\\1./g Wrapping up # Finally, I move the files generated by the python script into a directory named after the file and page number(s) the being procssed and then copy that directory and its contents into a backup directory.\nmkdir $NEW_DIR mv $FILE_BASENAME*.??? $NEW_DIR cp -R $NEW_DIR \u0026#34;${NEW_DIR}.bak\u0026#34; Pasting metadata # The data entry for this workflow happens in Google Sheets, so I use Miller to convert the generated CSV file to TSV for easier pasting into Sheets. I could probably go back and have the script spit out TSV instead of CSV.\nmlr --c2t --headerless-csv-output cat 74_2_{PAGE_NUM}.csv | pbcopy --c2t tells Miller to convert CSV to TSV, and --headerless-csv-output ignores the first line of the file.\nUp next # As you can probably tell, this script is really just a harness for the get_summaries.py script that does the bulk of the work. That script will be explained in the next post.\n"},{"id":2,"href":"/portfolio/docs/technology/metadata/2022-11-24-get-summaries/","title":"3. get_summaries.py","section":"Metadata Workflow Series","content":" get_summaries.py # I hope to convert this file to ipynb format so interested folks can try the code out on their own. The code is currently published on GitHub.\nEnvironment Setup # The Shebang Line # We add a shebang line in case we want to make this file executable.\n#!/usr/bin/env python Necessary Libraries # Now we import the libraries we need.\nimport argparse import sys import re import csv from pathlib import Path from more_itertools import chunked from PyPDF2 import PdfReader The role of each library is explained below:\nargparse: handles command-line argument parsing In this script, those command line arguments are the path to the PDF file we are processing, the page we will start processing from, and an optional argument for where we will stop processing. sys: handles a variety of interactions between python and its host system In this script, it is used once: to exit the program in case of a fatal error re: is python\u0026rsquo;s regular expression library In this script, it does the bulk of the work. csv: is python\u0026rsquo;s library for dealing delimited data files. In this script, it is used to write metadata from the bill summaries to a csv file. from pathlib import Path: imports the Path object for dealing with file paths from more_itertools import chunked: imports the chunked function from the more_itertools library more_itertools is not part of a standard python distribution and needs to be installed using pip. from PyPDF2 import PdfReader: is a third-party library for dealing PDF files that be installed using pip. Constants # (No) Magic Numbers # There are three numbers that play key roles in the script. To avoid having magic numbers, we make them constants with descriptive names.\nELEMENT_COUNT = ( 7 # full header, bill type, bill number, sponsor(s), action date, committee, text ) CONGRESS_POSITION = 2 SESSION_POSITION = 3 ELEMENT_COUNT: is adequately explained by the comment in the above code block. CONGRESS_POSITION: refers to starting index of the number in the name of the file passed to the script that refers to the congress (e.g., 74th) the summaries were written for. SESSION_POSITION: is like CONGRESS_POSITION, but for the session of Congress (e.g., second). The Month Dictionary # We use a python dictionary to map between names (or abbreviations, or bad OCR text) of months as found in the bill summaries and two character strings of digits that will be used in naming the files containing the extracted summaries.\nMONTHS = { \u0026#34;January\u0026#34;: \u0026#34;01\u0026#34;, \u0026#34;February\u0026#34;: \u0026#34;02\u0026#34;, \u0026#34;Feb.\u0026#34;: \u0026#34;02\u0026#34;, \u0026#34;March\u0026#34;: \u0026#34;03\u0026#34;, \u0026#34;Mar.\u0026#34;: \u0026#34;03\u0026#34;, \u0026#34;April\u0026#34;: \u0026#34;04\u0026#34;, \u0026#34;Apr.\u0026#34;: \u0026#34;04\u0026#34;, \u0026#34;May\u0026#34;: \u0026#34;05\u0026#34;, \u0026#34;June\u0026#34;: \u0026#34;06\u0026#34;, \u0026#34;July\u0026#34;: \u0026#34;07\u0026#34;, \u0026#34;August\u0026#34;: \u0026#34;08\u0026#34;, \u0026#34;September\u0026#34;: \u0026#34;09\u0026#34;, \u0026#34;October\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;November\u0026#34;: \u0026#34;11\u0026#34;, \u0026#34;December\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;Alay\u0026#34;: \u0026#34;05\u0026#34;, # bad OCR, should be \u0026#34;May\u0026#34; } Big, Ugly Regular Expressions # The script relies on two regular expressions to do its work. They\u0026rsquo;re big, they\u0026rsquo;re ugly, they\u0026rsquo;re poorly tested, and they match a lot more than they are intended to match, but this script would be useless without them. In a later draft of this post, and a later version of the script, I will rewrite the regular expressions with the verbose flag, which they obviously need. I decided to demonstrate the version of the regexes the script actually uses.\nThe Summary Header Pattern # This regular expression (usually) matches the first line of a bill summary and groups (see the second row of the table) parts of the line that will be extracted into a starter CSV metadata file. The abundance of punctuation options has to do primarily with inconsistency in how OCR recognizes the punctuation characters in the original text, but there is also some inconsistency in how the original text is punctuated.\nHEADER_PATTERN = re.compile( r\u0026#34;(((?:S|H)\\.? ?(?:R\\.?)? (?:J\\.? Res\\. ?)?)(\\w{1,5})\\.? ((?:M(?:rs?|essrs)\\.) .+?)(?:[;,:])? (\\w{1,9} \\d{1,2}[.,] \\d{4})[.—]? ?\\n?(?:\\(([\u0026#39;0-9a-zA-Z ]+)\\))?(?:\\.|.+\\.|:|.+:)?)\u0026#34;, re.MULTILINE, ) The regex captures the bill type, bill number, sponsors, date of introduction, committee, and text of the bill summary. It also captures the entire first line of the summary.\nThe Date Pattern # This regular expression is intended to match dates like the following: January 16, 1936. The year is optional.\nDATE_PATTERN = re.compile(r\u0026#34;([JFMASOND][a-z]{2,8}\\.?) (\\d{1,2})[-—.,;: ]( \\d{4})?\u0026#34;) Functions # Now we turn our attention to the functions that will be called in the main body of the script.\nArgument Parsing # Here, we use the argparse package to handle three command line arguments: the name of the file, the page to start from, and the page to end on.\ndef parse_args(): parser = argparse.ArgumentParser() parser.add_argument( \u0026#34;file_path\u0026#34;, help=\u0026#34;path to a bill digest PDF, e.g., ../74_2.pdf\u0026#34; ) parser.add_argument( \u0026#34;start_page\u0026#34;, help=\u0026#34;PDF page number (not printed page number) for where program should start working\u0026#34;, type=int, ) parser.add_argument( \u0026#34;end_page\u0026#34;, help=\u0026#34;last PDF page number (not printed page number) program should process\u0026#34;, type=int, nargs=\u0026#34;?\u0026#34;, ) return parser.parse_args() Naming the CSV data file # The CSV file we generate needs a meaningful name, e.g., 74_2_7_8.csv, that is, the Congress and Session (74_2, which we get from the file stem of the PDF file we\u0026rsquo;re processing, followed by the start page and the end page (if one is specified).\ndef name_output_file(file_stem, start_page, end_page): if end_page: return \u0026#34;_\u0026#34;.join([file_stem, str(start_page), str(end_page)]) + \u0026#34;.csv\u0026#34; else: return \u0026#34;_\u0026#34;.join([file_stem, str(start_page)]) + \u0026#34;.csv\u0026#34; Getting the Data # This is where most of the work happens. The function could and should probably be broken out into some smaller functions, but it works. See the comments in the function definition for details, and the moreitertools chunked documentation for information about the use of that function.\ndef extract_summaries_and_metadata(file_path, start_page, end_page): # Set up some empty containers for our data metadata = [] summaries = [] text_file_names = [] # Instantiate the PdfReader object reader = PdfReader(file_path) # Get the file stem, e.g., 74_2 file_stem = file_path.stem # Handle the presence of absence of an ending page number if end_page == None: end = start_page + 1 else: end = end_page # Iterate over page numbers for i in range(start_page, end): # Get the text from each page page_text = reader.pages[i].extract_text() # Find the first line that matches HEADER_PATTERN from above first_header_pos = re.search(HEADER_PATTERN, page_text).start() # Get the text from that point on page_text = page_text[first_header_pos:] # Split the text based on the header pattern raw_summaries = re.split(HEADER_PATTERN, page_text)[1:] # Group the summaries raw_summaries = list(chunked(raw_summaries, ELEMENT_COUNT)) for item in raw_summaries: # Give a name to each grouped piece of each summary header, bill_type, bill_number, sponsor, date, committee, text = item # Normalize the bill type formatting formatted_bill_type = bill_type.strip() # Normalize bill type for the summary text file name lower_bill_type = bill_type.lower().replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;).replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) # Concatenate the summary header and summary text summary = header + text # Add a row of metadata to the list metadata.append( [formatted_bill_type, bill_number, sponsor, date, committee] ) # Add a summary to the list summaries.append(summary) # Add the name of the text file to the list text_file_names.append(f\u0026#34;{file_stem}_{lower_bill_type}{bill_number}\u0026#34;) # return a tuple of lists of data return (metadata, summaries, text_file_names) Dates # Here\u0026rsquo;s an example of a bill summary in the original PDF:\nAnd here\u0026rsquo;s what it looks like after processing:\nS. 1019. Mr. King; January 15, 1935 (District of Columbia).\nAs passed by the Senate, February 12, and referred to House Committee on the District of Columbia. February 15, 1935:\nRequires real estate brokers and salesmen in the District of Columbia (including nonresidents but excluding auctioneers, banks, trust companies, building and loan associations or land-mortgage or farm-loan associations) to secure annual licenses from a real estate commission hereby established (composed of three members—two appointed by the Commissioners and the assessor ex-officio). Applicants for licenses must be 21 years old, able to read and write English and must give proof of trustworthiness and competence in the business (not required if applicant has 2 years experience as a broker, etc., or in connection with real estate business in the District of Columbia). Bond required—$2,500 for brokers, $1,000 for salesmen—running to the District of Columbia; and a fee for broker’s license of $25 or $5 for salesman’s license. License may be revoked by the Commission, upon its own motion or on a verified complaint, after a hearing, for any fraudulent or dishonest dealing or conviction of a crime involving fraud or dishonesty. Revocation of a broker’s license suspends the license of every salesman under him.\nYou\u0026rsquo;ll notice that there are multiple dates specified. The date from the first line goes into the CSV file, but we want the latest action to go into the name of the text file for each summary. The function below handles that.\ndef format_latest_action_dates(summaries): actions = [] date_tags = [] intro_dates = [] for summary in summaries: # Break the summary text into lines lines = summary.splitlines() # Get the date on the first intro_dates.append(re.findall(DATE_PATTERN, lines[0])[0]) # If there is more than one line in the summary, store the second line, otherwise, store an empty string if len(lines) \u0026gt; 1: actions.append(lines[1]) else: actions.append(\u0026#34;\u0026#34;) # Process the second line of each summary for action in actions: # Get the whole date the bill was introduced intro_date = intro_dates[actions.index(action)] # Get the year from that date intro_year = intro_date[2] # Find all the dates in the second line of the summary dates = re.findall(DATE_PATTERN, action) # If there are dates in the second line, handle that if dates: # Sometimes no years are specified in the second line of the summary, as in the example above default_year = dates[0][2] or intro_year date = list(dates[-1]) if date[2] == \u0026#34;\u0026#34;: date[2] = default_year # Default to the year the bill was introduced else: date = list(intro_date) # Convert the month text to a two digit numerical string date[0] = MONTHS[date[0]] # Create the date string that will go into the name of the text file date_tag = f\u0026#34;{date[2].strip()}{date[0]}{date[1].zfill(2)}\u0026#34; date_tags.append(date_tag) return date_tags Output File Names # Here\u0026rsquo;s how we name the text file associated with each summary.\ndef format_output_files(text_file_names, date_tags): # If the number of text file base names and the number of date tags doesn\u0026#39;t match, we\u0026#39;ve got a problem if len(text_file_names) != len(date_tags): print(\u0026#34;Text files names list and date tags list are not the same length.\u0026#34;) sys.exit(1) # Zip the text file base names and date strings together, join them with \u0026#34;_\u0026#34;, and add the \u0026#34;.txt\u0026#34; extension filenames = map( lambda name: \u0026#34;_\u0026#34;.join(name) + \u0026#34;.txt\u0026#34;, list(zip(text_file_names, date_tags)) ) return list(filenames) Writing out the files # Here\u0026rsquo;s how we write the files to disk.\nWriting the CSV File # This is decently straightforward thanks to the csv module in the python standard library.\ndef write_metadata(metadata, output_file_name, congress): headers = [ \u0026#34;congress\u0026#34;, \u0026#34;session\u0026#34;, \u0026#34;pl_num\u0026#34;, \u0026#34;bill_type\u0026#34;, \u0026#34;bill_number\u0026#34;, \u0026#34;sponsor\u0026#34;, \u0026#34;committee\u0026#34;, \u0026#34;action_red\u0026#34;, \u0026#34;summary_version_code\u0026#34;, \u0026#34;report_number\u0026#34;, \u0026#34;action\u0026#34;, \u0026#34;action_date\u0026#34;, \u0026#34;associated_summary_file\u0026#34;, \u0026#34;questions_comments\u0026#34;, ] with open(output_file_name, \u0026#34;w\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=headers) writer.writeheader() for row in metadata: writer.writerow( { \u0026#34;congress\u0026#34;: congress, \u0026#34;session\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pl_num\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bill_type\u0026#34;: row[0], \u0026#34;bill_number\u0026#34;: row[1], \u0026#34;sponsor\u0026#34;: row[2], \u0026#34;committee\u0026#34;: row[4] or \u0026#34;N/A\u0026#34;, \u0026#34;action_red\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;summary_version_code\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;report_number\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;Introduced\u0026#34;, \u0026#34;action_date\u0026#34;: row[3], \u0026#34;associated_summary_file\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;questions_comments\u0026#34;: \u0026#34;\u0026#34;, } ) Writing the Summary Files # This is the most straightforward function in the script. Essentially, we iterate over the summaries and match them up with the text file names we created.\ndef write_summaries(summaries, text_file_names): for i in range(len(summaries)): with open(text_file_names[i], \u0026#34;w\u0026#34;) as f: f.write(summaries[i]) Putting it all together # Now we actually do what we set out to do.\nif __name__ == \u0026#34;__main__\u0026#34;: args = parse_args() file_path = args.file_path start_page = args.start_page start_page_index = start_page - 1 end_page = args.end_page file_path = Path(file_path) file_stem = file_path.stem output_file_name = name_output_file(file_stem, start_page, end_page) congress = file_stem[:CONGRESS_POSITION] session = file_stem[SESSION_POSITION] metadata, summaries, text_file_names = extract_summaries_and_metadata( file_path, start_page_index, end_page ) action_dates = format_latest_action_dates(summaries) output_file_names = format_output_files(text_file_names, action_dates) write_metadata(metadata, output_file_name, congress) write_summaries(summaries, output_file_names) Up next # Now it\u0026rsquo;s time for the manual work. In the next post, I\u0026rsquo;ll detail how I use GNU Emacs to review each text file and Google Sheets to input additional metadata about each bill.\n"},{"id":3,"href":"/portfolio/docs/technology/metadata/2022-11-25-emacs-workflow/","title":"4. Emacs Workflow","section":"Metadata Workflow Series","content":" Emacs Workflow # We\u0026rsquo;ve run prep.sh (and with it, get_summaries.py), so now we\u0026rsquo;ve got a directory full of text files to review. This is where GNU Emacs comes in.\nInitially, I was doing all of this work on the command line, using iTerm2 on my Mac. I had a script that ran aspell on each text file before opening Neovim for each file for manual review. It worked fine, but it wasn\u0026rsquo;t very flexible, and to be honest, I\u0026rsquo;ve always wanted to become proficient in Emacs. This project presented me with a great opportunity to it a daily driver.\nIn the interest of keeping this post shorter than the last one, I\u0026rsquo;ll describe the workflow I landed on and not all the steps along the way.\nFinding the Directory # Within Emacs, I use C-x d to navigate to the directory created by prep.sh, e.g., 74_2_199. This opens Dired, the Emacs Directory Editor, where I next run dired-omit-mode with C-x M-o to hide uninteresting files (e.g., backup files that end with ~ and .bak).\nEditing the Files # Now, I use n and p to navigate between the listed files, hitting the enter key to open the file I\u0026rsquo;d like to edit. Within each file, I run M-x ispell with aspell as the backend for some automated spell-checking. I remove any weird entities introduced by OCR fix up line and paragraph breaks.\nThere are two really important elements of this step:\nEnter metadata about bill actions besides introduction into the spreadsheet Make sure the name of the file corresponds to the date of the latest action on the bill. Putting the data into Google Sheets is pretty straightforward, except for a couple of steps at the end of this element of the workflow that I\u0026rsquo;ll detail later in the post.\nIf I need to edit the name of the file, I do this back in the Dired buffer using the R key, making sure to change the name in the spreadsheet as well.\nFinalizing the spreadsheet data # Before wrapping up this element of the workflow, I do two things:\nI type the date into the spreadsheet as written in the original document, e.g., May 1, 1936, but I eventually reformat it into ISO 8601 using Format \u0026gt; Number in Google Sheets for some easier processing in SQLite later in the workflow. I fill down the Session column with the formula =if(year(L2088)=1935, 1, 2) (the L column is the Action Date column). Cleaning up and next steps # Finally within Emacs I run the script cleanup.sh, which will be explained in the next post, using the M-x ! key with the argument ../cleanup.sh.\n"},{"id":4,"href":"/portfolio/docs/technology/metadata/2022-11-25-cleanup/","title":"5. cleanup.sh","section":"Metadata Workflow Series","content":" cleanup.sh # With the text files edited, it\u0026rsquo;s time to clean up a little bit. That\u0026rsquo;s what cleanup.sh is for.\nAs usual, we start with the shebang line.\n#!/usr/bin/env zsh We remove all of the no longer necessary backups. The -f makes sure there are no complaints in case no backups exist.\n# remove temporary files rm -f *.bak rm -f *~ This section isn\u0026rsquo;t really necessary, but I added in to make working with one-off awk commands a little easier (tabs as delimiters are much easier to deal with than commas).\n# tsv file for easier command line text processing filepath=`realpath *.csv` dirname=`dirname $filepath` basename=`basename $filepath \u0026#39;.csv\u0026#39;` mlr --c2t cut -o -f bill_type,bill_number,sponsor,action_date,committee $filepath \u0026gt; \u0026#34;$dirname/$basename.tsv\u0026#34; echo \u0026#34;Created $dirname/$basename.tsv\u0026#34; Miller really shines here. awk could do the same thing, but with Miller I can use the names of the columns. In awk, I\u0026rsquo;d have to remember which numeric field variable, e.g., $1 corresponds to which column.\nHere, we add the names of all the text files in the directory to the clipboard for pasting into the spreadsheet.\nls *.txt | pbcopy echo \u0026#34;Filenames copied to clipboard.\u0026#34; Now it\u0026rsquo;s time to get rid of the backup directory created by prep.sh if it exists.\nPWD=$(pwd) if [[ -d \u0026#34;${PWD}.bak\u0026#34; ]] then rm -rf \u0026#34;${PWD}.bak\u0026#34; echo \u0026#34;Backup removed.\u0026#34; fi Finally, we move the directory up to its final location on my hard drive.\nmv $PWD $(dirname $(dirname $PWD)) echo \u0026#34;${PWD} moved.\u0026#34; cd .. Up next # Now it\u0026rsquo;s time for a little data munging with the script update-tsv-data.sh.\n"},{"id":5,"href":"/portfolio/docs/technology/metadata/2022-12-04-meta-data-sette/","title":"6. (meta)data(sette)","section":"Metadata Workflow Series","content":" (meta)data(sette) # Intro # After running cleanup.sh I like to make a local copy of it as a tab-separated file I can do various things with, including storing it in SQLite database with Datasette which, among other things, allows me to link the metadata with their associated text files.\nThe Code # As usual, it\u0026rsquo;s a shell script.\n#!/usr/bin/env zsh I use wget to save the data to my local machine. Getting the URL right took a little Googling and turned up this great answer on Stack Overflow. The spreadsheet has to be published to the web in order for this to work.\nwget -q -O ./all_data.tsv https://docs.google.com/spreadsheets/d/e/2PACX-1vQkoFBuqjcBX-a_xT3IfTmLmUJ0LZme8Mj16sIdqqEC9Z_vPmyHJKiTCRrZHPaSsxI-4lKaLkWD_XSk/pub\\?gid\\=0\\\u0026amp;single\\=true\\\u0026amp;output\\=tsv The Database # I then (re)create the SQLite database containing the bill metadata and the summary text files using Simon Willison\u0026rsquo;s extraordinarily easy to use sqlite-utils.\nFirst, I drop the existing tables that hold the core data.\nsqlite-utils drop-table summaries.db actions sqlite-utils drop-table summaries.db files Then I recreate the actions table, which is the table that holds the bill metadata\nsqlite-utils insert summaries.db actions all_data.tsv --tsv -d I rename the columns for from the spreadsheet for use in the database. There may be a better way to do this, but this works.\nsqlite-utils transform summaries.db actions --rename \u0026#34;Congress\u0026#34; \u0026#34;congress\u0026#34; --rename \u0026#34;Session\u0026#34; \u0026#34;session\u0026#34; --rename \u0026#34;PL Num (if applicable)/PVTL Num\u0026#34; \u0026#34;pl_num\u0026#34; --rename \u0026#34;Bill Type\u0026#34; \u0026#34;bill_type\u0026#34; --rename \u0026#34;Bill Number \u0026#34; \u0026#34;bill_number\u0026#34; --rename \u0026#34;Sponsor\u0026#34; \u0026#34;sponsor\u0026#34; --rename \u0026#34;Committee\u0026#34; \u0026#34;committee\u0026#34; --drop \u0026#34;Action Code\u0026#34; --drop \u0026#34;Summary Version Code\u0026#34; --drop \u0026#34;Report Number\u0026#34; --rename \u0026#34;Action\u0026#34; \u0026#34;action\u0026#34; --rename \u0026#34;Action Date\u0026#34; \u0026#34;action_date\u0026#34; --rename \u0026#34;Associated Summary Text File Name\u0026#34; \u0026#34;summary_text_file_name\u0026#34; --rename \u0026#34;Questions/Comments\u0026#34; \u0026#34;questions_comments\u0026#34; Now I insert all the text file summaries on my local machine into the database.\nsqlite-utils insert-files --text summaries.db files 74*/*.txt This ends up saving the directory name and file name in the files tables\u0026rsquo;s path column. We just want the file name since that\u0026rsquo;s what is recorded in the actions table. The following fixes that for us.\nsqlite-utils convert summaries.db files path \u0026#39;value.split(\u0026#34;/\u0026#34;)[1]\u0026#39; Now we can link up the two tables\nsqlite-utils add-foreign-key summaries.db actions summary_text_file_name files path --ignore and publish the database as an app on Heroku.\ndatasette publish heroku summaries.db --name \u0026#34;llc\u0026#34; --tar \u0026#34;/usr/local/bin/gtar\u0026#34; --install=datasette-saved-queries Local Copies of the Data # Next I massage the data a little bit to allow for some simple analysis on the command line with MIller. The awk command drops the columns that don\u0026rsquo;t have any data in them and the Miller command fills empty spots in the Sponsor and Committee columns, making the data bit tidy-er.\nawk \u0026#39;BEGIN{FS=OFS=\u0026#34;\\t\u0026#34;}{print $1, $2, $4, $5, $6, $7, $11, $12}\u0026#39; all_data.tsv | mlr --tsv fill-down -f Sponsor,Committee then cat \u0026gt; filled_metadata.tsv Finally, to allow for easier reading of the spreadsheet by humans, the following awk command puts a blank line between each bill. $5 refers to the Bill Number column.\n# insert blank lines between bills for pasting into final spreadsheet awk \u0026#39;BEGIN{FS=\u0026#34;\\t\u0026#34;} {cur=$5} NR\u0026gt;1 \u0026amp;\u0026amp; cur!=prev {print \u0026#34;\u0026#34;} {prev=cur; print}\u0026#39; all_data.tsv \u0026gt; spaced_metadata.tsv Next Time # This wraps up the core workflow, so now it\u0026rsquo;s time to move on how I actually use the Datasette web interface and SQL queries to check metadata quality.\n"},{"id":6,"href":"/portfolio/docs/outcomes/articulate/","title":"Critically Articulating the Philosophy, Principles, and Ethics of Library and Information Science","section":"Program Learning Outcomes and Work Samples","content":" Critically articulating the philosophy, principles, and ethics of library and information science # "},{"id":7,"href":"/portfolio/docs/outcomes/evaluate/","title":"Evaluating Technology-Mediated Access in Library and Information Services","section":"Program Learning Outcomes and Work Samples","content":" Evaluating technology-mediated access in library and information services # "},{"id":8,"href":"/portfolio/docs/outcomes/practice/critcat/","title":"LS 501: Critical Cataloging: An LIS Hot Topic","section":"Practicing Principles of Social and Cultural Justice","content":"William Blackerby\nDr. Jeff Weddle\nLS 501-925\n6 October 2021\nCritical Cataloging: An LIS Hot Topic\nAs part of their university libraries\u0026rsquo; commitment to equity, diversity, inclusion, and anti-racism, metadata specialists at the University of Washington published a statement on harmful language in catalog records and archival finding aids in August of 2021 (Casey et al., \u0026ldquo;Words Matter\u0026rdquo;). UW Libraries\u0026rsquo; publication of their \u0026ldquo;Critical Cataloging and Archival Description\u0026rdquo; statement takes place in the context of the ongoing, broader national conversation about equity, diversity, and inclusion prompted by systemic police violence and killings of unarmed Black people and brought to a head by the murder of George Floyd by Minneapolis police officer Derek Chauvin in May of 2020. The statement is one manifestation of UW Libraries\u0026rsquo; commitment to equity, diversity, and inclusion and represents a commitment to transparency in the library\u0026rsquo;s resource description process. UW Libraries are, of course, not the first to make such a statement; in fact, their statement references similar work undertaken in recent years at other academic libraries including those at Drexel University, Duke University, the University of Iowa, MIT, and Temple University. At issue in these statements is the use of racist, sexist, and otherwise harmful terms in standardized, controlled vocabularies such as the Library of Congress Subject Headings (LCSH) used to describe library resources. The work of critically examining and remediating the language used in classification and cataloging is known as critical cataloging, one element of the broader field of critical librarianship.\nMyriad examples of harmful language used in subject headings and classifications can be drawn from the library catalog, but one recent, concrete example is the Library of Congress Subject Heading \u0026ldquo;Illegal aliens.\u0026rdquo; In 2014, students at Dartmouth College \u0026ldquo;petitioned the Library of Congress (LC) to change the catalog subject heading \u0026lsquo;illegal aliens\u0026rsquo; to \u0026lsquo;undocumented immigrants\u0026rsquo;\u0026rdquo; (Ford, \u0026ldquo;Conscientious Cataloging\u0026rdquo;). In 2016, after the ALA passed a resolution urging the Library of Congress to comply with this petition, LC\u0026rsquo;s Policy and Standards Division cancelled the heading \u0026ldquo;Illegal aliens,\u0026rdquo; deciding to replace it with the two headings \u0026ldquo;Noncitizens\u0026rdquo; and \u0026ldquo;Unauthorized immigration\u0026rdquo; (\u0026ldquo;Library of Congress to Cancel the Subject Heading \u0026lsquo;Illegal Aliens\u0026rsquo;\u0026rdquo;). However, in June of 2016, because of political pressure from the Republican-led Congress, LC kept the cancelled heading, which is still in place today (Ford).\nIn accordance with the ALA core value of diversity, librarians have confronted the issue head on in their own institutions by, where possible, replacing the \u0026ldquo;Illegal aliens\u0026rdquo; subject heading with \u0026ldquo;Undocumented immigrants\u0026rdquo; and where not possible, placing the two terms side by side. According to Sol López, a librarian who implemented these changes at two academic libraries, changes such as these made at the local level communicate inclusion to \u0026ldquo;library users who may have undocumented status\u0026rdquo; (Ford). At the Lawrence Public Library in Kansas, Kate Ray and Emily McDonald made similar changes after viewing a documentary film titled Change the Subject about Dartmouth students\u0026rsquo; efforts to change the LCSH*.* Ray and McDonald completely removed the \u0026ldquo;Illegal alien\u0026rdquo; subject heading from their library\u0026rsquo;s catalog due to their acute awareness of the harm that such language can cause library patrons. As Ray says, patrons \u0026ldquo;think it\u0026rsquo;s our decision to use [harmful language], and that that term is a judgment on them by the people who work at the library\u0026rdquo; (Ford). Ray and McDonald\u0026rsquo;s sense of \u0026ldquo;responsibility to make [their] library as welcoming to as many people as possible\u0026rdquo; is a clear reflection of the ALA core value of access, which states that \u0026ldquo;information resources\u0026hellip;should be readily, equally, and equitably accessible to all library users\u0026rdquo; (Ford; \u0026ldquo;Core Values of Librarianship\u0026rdquo;).\nIn July of 2016, the same year the Library of Congress announced its decision to cancel the \u0026ldquo;Illegal alien\u0026rdquo; subject heading and then effectively reversed its decision by not implementing it, LC approved a proposal to add the subject heading \u0026ldquo;Asexuality\u0026rdquo; (Watson 547-548). Watson\u0026rsquo;s article \u0026ldquo;\u0026lsquo;There was Sex but no Sexuality*:\u0026rsquo; Critical Cataloging and the Classification of Asexuality in LCSH\u0026rdquo; is a valuable contribution to the literature for a host of reasons, including its review of literature on critical cataloging and in-depth recounting of the process by which \u0026ldquo;Asexuality\u0026rdquo; was made an official Library of Congress Subject Heading. Of greater interest, though, is the personal story of Paige Crowl\u0026rsquo;s own experience as an MLIS student searching for information about asexuality.\nCrowl\u0026rsquo;s search of her university catalog for \u0026ldquo;asexuality\u0026rdquo; resulted in references to \u0026ldquo;biological works on asexual reproduction and psychological works on disorders of sexual desire\u0026rdquo; (qtd. in Watson 550). Perhaps a librarian could have aided, but Crowl was \u0026ldquo;too embarrassed to ask a librarian for help, unsure [she] could even explain the information need [she] had to [her]self\u0026rdquo; (qtd. in Watson 550). Crowl\u0026rsquo;s story is a clear example of the harm that can be caused by the decisions that create library cataloging and classification systems and the importance of working to ensure that such systems avoid harmful language. Even if a friendly, empathetic librarian with expert search skills is available to help, library patrons may not be aware of this fact, or may be too scared to ask for help, meaning that the catalog is often the first and sometimes the only interface patrons have to the library\u0026rsquo;s resources. If patrons have a harmful or offensive experience interacting with the catalog, the librarians responsible for stewarding that catalog have, unintentionally or not, violated their core professional value of access.\nThe publication of UW Libraries\u0026rsquo; statement on \u0026ldquo;Critical Cataloging and Archival Description\u0026rdquo; is just one more recent point on the long and continuing timeline of the critical study of how information is organized, categorized, classified, and cataloged in LIS scholarship. In a tweet on October 1, 2021, Violet Fox, a leader in the critical cataloging movement, reminded her followers about Frances Yocom, a white librarian who \u0026ldquo;wrote about the lack of subject headings for materials about African-Americans in 1940\u0026rdquo; (@violetbfox). Fox\u0026rsquo;s tweet quoted another by librarian Harvey Long about Annette Phinazee, a Black librarian whose \u0026ldquo;1961 Columbia University dissertation was a critical examination of LoC and its cataloging practices\u0026rdquo; (@harvlong). Along with Yocom, Watson mentions Black librarian Dorothy Porter who, in building the Moorland-Spingarn Research Center for black history and culture at Howard University, \u0026ldquo;instead of using the Dewey system,\u0026rdquo; which severely limited how works by Black authors could be classified, \u0026ldquo;classified works by genre and author to highlight the foundational role of black people in all subject areas\u0026rdquo; (Watson 551; Nunes, \u0026ldquo;Remembering the Howard University Librarian Who Decolonized the Way Books Were Catalogued\u0026rdquo;). Watson also notes that \u0026ldquo;widespread scholarly discussion\u0026rdquo; of the \u0026ldquo;Western and white-centric nature of cataloging\u0026rdquo; began \u0026ldquo;to develop after the publication of Sanford \u0026lsquo;Sandy\u0026rsquo; Berman\u0026rsquo;s 1971 book Prejudices and Antipathies: A Tract on the LC Subject Heads Concerning People\u0026rdquo; (Watson 551).\nThe decisions made and actions taken by the practicing librarians mentioned above to revise the language in LCSH and, when that wasn\u0026rsquo;t possible, make changes within their own spheres of influence reflect current scholarship in the field. These are what Olson would call \u0026ldquo;local, partial, and dynamic changes\u0026rdquo; necessary to make the \u0026ldquo;standards permeable to meet diverse information needs\u0026rdquo; (Olson 661). Adler notes that \u0026ldquo;the way we decide to classify something says a lot about how we want to remember it, but a closer examination also says a great deal about what we\u0026rsquo;re willing to forget or disavow\u0026rdquo; (Adler 553). To their credit, the librarians who brought about the addition of asexuality to LCSH and who removed the \u0026ldquo;Illegal aliens\u0026rdquo; subject heading from their local catalogs showed that they were not willing to disavow the humanity of those who do not benefit from \u0026ldquo;the discourses that privilege certain norms\u0026rdquo; (Adler and Harper 57). In fact, they engaged the ethical responsibility they bear thanks to the power they control to organize information (Adler and Harper 58).\nAddressing harmful language in library catalogs must take place collaboratively with the communities libraries serve. To that end, UW Libraries have laid out a clear process that is respectful of patron privacy by which patrons can report offensive or harmful language they encounter in the catalog (Casey et al.). Metadata specialists are responsible for mountains of data and are hampered by budget restraints and limited time in the day. In the face of these obstacles, patron participation in the remediation of harmful language in the catalog is a welcome and necessary solution.\nWhile criticism of LCSH dates back decades, it will always be a timely issue. Language is always changing, so metadata specialists will always need to respond to such change by striving to ensure that resource descriptions are up-to-date and devoid of, as far as is possible, harmful terminology. As Olson says, the \u0026ldquo;\u0026lsquo;better and quicker and cheaper\u0026rsquo;\u0026rdquo; of standardized metadata \u0026ldquo;is always at a price, and the price is the violent reshaping of objects to fit the preconceptions of the knowing subject\u0026rdquo; (663). One hopes that the field now recognizes and is willing to constantly reexamine the diversity of knowing subjects and is \u0026ldquo;getting out of the way enough for communities to determine how they want to best document themselves\u0026rdquo; (Matienzo 2015). The movement by librarians in institutional settings like academic and public libraries is a promising, though admittedly imperfect, start. Without this work, work that must always be returned to, libraries run the risk of further dehumanizing their patrons and obviating the reason for their existence in the first place.\nWorks Cited\n@harvlong. \u0026ldquo;Her name was Annette Phinazee. By all accounts, she was the GOAT. Her 1961\nColumbia University dissertation was a critical examination of the LoC and its cataloging practices.\u0026rdquo; Twitter, 30 Sept. 2021, 1:04 p.m., https://twitter.com/harvlong/status/1443637915519754243.\n@violetbfox. \u0026ldquo;Reminder that #critcat/radical cataloging history doesn't begin \u0026amp; end with\nBerman. See also Frances Yocom who wrote about the lack of subject headings for materials about African-Americans in 1940.\u0026rdquo; Twitter, 1 Oct. 2021, 2:09 a.m., https://twitter.com/violetbfox/status/1443835511777472528.\nAdler, Melissa. \u0026quot;Afterword: The Strangeness of Subject Cataloging.\u0026quot; Library Trends, vol. 68,\nno. 3, 2020, pp. 549-556. Project MUSE, http://doi.org/10.1353/lib.2020.0005.\nAdler, Melissa and Lindsey M. Harper. \u0026ldquo;Race and Ethnicity in Classification Systems: Teaching\nKnowledge Organization from a Social Justice Perspective.\u0026rdquo; Library Trends, vol. 67, no. 1, 2018, pp. 52-73. Project MUSE, \u0026gt; https://doi.org/10.1353/lib.2018.0025.\nCasey, Conor M., Erin Grant, Keiko Hill, Kat Lewis, Crystal Rodgers. \u0026ldquo;Words Matter: Critical\nCataloging and Archival Description at the University of Washington Libraries.\u0026rdquo; UW Libraries Blog, https://sites.uw.edu/libstrat/2021/09/07/words-matter-critical-cataloging-and-archival-description-at-the-university-of-washington-libraries/. Accessed 6 October 2021.\n\u0026ldquo;Core Values of Librarianship.\u0026rdquo; American Library Association, 26 July 2006,\nhttps://www.ala.org/advocacy/intfreedom/corevalues. Accessed 6 October 2021.\nFord, Anne. \u0026ldquo;Conscientious Cataloging.\u0026rdquo; American Libraries, 1 Sept. 2020,\nhttps://americanlibrariesmagazine.org/2020/09/01/conscientious-cataloging/. Accessed 6 October 2021.\n\u0026ldquo;Library of Congress to Cancel the Subject Heading \u0026lsquo;Illegal Aliens.\u0026rsquo;\u0026rdquo; Library of Congress, 22\nMarch 2016, https://www.loc.gov/catdir/cpso/illegal-aliens-decision.pdf. Accessed 6 October 2021.\nMatienzo, Mark A. \u0026quot;To Hell With Good Intentions: Linked Data, Community, and the Power to\nName.\u0026quot; matienzo.org, 14 Nov. 2015, https://matienzo.org/2016/to-hell-with-good-intentions/. Accessed 6 October 2021.\nNunes, Zita Cristina. \u0026ldquo;Remembering the Howard University Librarian Who Decolonized the\nWay Books Were Catalogued.\u0026rdquo; Smithsonian Magazine, 26 Nov. 2018, https://www.smithsonianmag.com/history/remembering-howard-university-librarian-who-decolonized-way-books-were-catalogued-180970890/. Accessed 6 Oct. 2021.\nOlson, Hope A. \u0026ldquo;The Power to Name: Representation in Library Catalogs.\u0026rdquo; Signs, vol. 26, no. 3,\n2001, pp. 639-668.\nWatson, Brian M. \u0026ldquo;\u0026lsquo;There was Sex but no Sexuality*:\u0026rsquo; Critical Cataloging and the Classification\nof Asexuality in LCSH.\u0026rdquo; Cataloging \u0026amp; Classification Quarterly, vol. 58, no. 6, 2020, pp. 547-565.\n"},{"id":9,"href":"/portfolio/docs/outcomes/articulate/finalessays501/","title":"LS 501: Final Essays","section":"Critically Articulating the Philosophy, Principles, and Ethics of Library and Information Science","content":"William Blackerby\nLS 501-925\nDr. Jeff Weddle\nDecember 5, 2021\nFinal Essay Exam # Two Essential ALA Core Values # The two ALA core values that are most critical for the profession right now are Access and the Public Good. The amount of information available to our communities continues to grow and contribute to increasing both information overload and misinformation. Reputable information is often difficult to find or inaccessible due to paywalls, discoverability challenges, institutional and socio-cultural biases, and other impediments. These two complementary core values offer an antidote to this situation, and librarians have an essential role to play in helping their communities both find the information they need and make sense of it.\nIt is easy to feel an acute sense of the loss of a shared understanding of the public good these days. A screenshot of an old tweet by the user @_Amanda_Killian, whose account is no longer findable, has been circulating online recently and expresses this sentiment with these words: \u0026ldquo;Libraries literally aren\u0026rsquo;t just a place to obtain books for free. They\u0026rsquo;re one of the few public spaces left in our society where you\u0026rsquo;re allowed to exist without the expectation of spending money\u0026rdquo; (@AletheaKontis). Along the same lines, Shannon Mattern makes the case that libraries are essential social infrastructure (a public good), saying \u0026ldquo;Most people who use the [library] building are not going there just to read a book or watch a film; many of them probably do not have any definite purpose at all. They go just to be part of the community in the building\u0026rdquo; (Mattern). This recalls Jürgen Habermas\u0026rsquo;s notion of the public sphere, which Tufekçi interprets \u0026ldquo;as the location and place in which rational arguments about matters concerning the public, especially regarding issues of governance and the civics can take place, freed from constraints of status and identity\u0026rdquo; (Tufekçi). Alstad and Curry state that \u0026ldquo;As a physical place, the public library exemplifies the public sphere\u0026rdquo; (11). Too much in our society is an individual\u0026rsquo;s worth connected to the value they contribute to the economy, which determines their status and identity. By holding up the Public Good as a core value, librarians push back against this tide and play an essential societal role reminding people that their value is not determined by their net worth, family, or job and that their voices matter in the public sphere.\nThere are many ways of looking at the core value of Access, but the most important way right now is to continually critically examine the language we use to make the items in our collections accessible. This has long been an area of interest and discussion in LIS, but Olson\u0026rsquo;s work makes it strikingly clear that our systems for organizing and naming our resources have a profound influence on \u0026ldquo;access to information outside of the cultural mainstream and about groups marginalized in our society\u0026rdquo; (640). In other words, access to information about topics outside of the cultural mainstream or about marginalized members of society often requires the use of harmful language. If libraries and librarians are to uphold their value that library resources \u0026ldquo;should be readily, equally, and equitably accessible to all library users,\u0026rdquo; they must understand that responsible librarianship demands that its practitioners constantly seek ways to improve this situation (\u0026ldquo;Core Values of Librarianship.\u0026rdquo;). It is essential to making the public sphere a welcoming place for all.\nLibraries and Neutrality # As a straight, white, cisgender male, privately educated from my pre-kindergarten days through my undergraduate degree, it has always been the norm for me to view neutrality as the safe, comfortable default. This is because I am a member of dominant groups, and as Dempsey states, \u0026ldquo;to be \u0026rsquo;neutral\u0026rsquo; can actually mean to be aligned with dominant perspectives\u0026rdquo; (Dempsey). Neutrality can also connote harmlessness, and I believe harmlessness is an implicitly stated ideal for libraries and librarians. That is, librarians and library institutions do not wish to harm their patrons. They exist, after all, to help. However, assuming a so-called neutral or objective position can inadvertently harm library patrons who are not members of dominant groups. Libraries and librarians, even when they choose neutrality, are not neutral.\nAdler and Harper state that \u0026ldquo;The very possibility of the idea of library neutrality exists because of the belief in the objectivity of library science. The cost of such framing is not only an erasure of discordant voices, but perhaps more important, a depoliticization of the library space\u0026rdquo; (57). When libraries choose neutrality, they non-neutrally erase discordant voices who, in some cases, are people who do not belong to dominant groups or hold dominant perspectives. Gibson, et al., more damningly frame \u0026ldquo;neutrality as a practice in structural oppression of marginalized groups, as it is characterized by disengagement from (as opposed to active engagement with) crises within communities of color\u0026rdquo; and go on to say that \u0026ldquo;choosing neutrality (or disengagement) in time of conflict is choosing to maintain status quo at the expense of one portion of a community\u0026rdquo; (754). Neutrality implies the absence of antagonism and conflict, but Seale, strikingly, says that \u0026ldquo;antagonism and conflict are what make democracy possible; a lack of dissent signifies the imposition of authoritarian order\u0026rdquo; (590). By this assessment, if librarians are to take seriously their core value of Democracy, they must reject neutrality outright.\nThat said, I do not take a rejection of neutrality to require that libraries and librarians take a position on the issue du jour or engage in partisanship. Rather, I borrow an operative word from Gibson et al.\u0026rsquo;s analysis: disengagement. If neutrality is disengagement, then a rejection of neutrality is engagement. Engagement does not immediately imply the taking of a position. Instead, it asserts an expectation that engaged librarians will inform themselves and thus work to assist their patrons to inform themselves as well. This engagement, especially among librarians, as self-informing and continual reflection will necessarily lead to making the library a more equitable space. Rejection of neutrality does not entail a rejection of harmlessness, which may be better termed as safety. In fact, rejecting neutrality and embracing engagement demonstrate an endorsement of psychological, existential safety for all patrons, especially those who are members of marginalized groups.\nLibraries and librarians are either actively not neutral or passively not neutral. Since libraries cannot be neutral, it is the responsibility of libraries and librarians to take the active route if they wish to uphold their professional values and serve their patrons to the best of their ability.\nInformation Services and Diverse Communities in a Global Society # Without communities, there are no libraries. As Lankes says, \u0026ldquo;All libraries, whether public or private, large or small, should be about communities, those they serve and are part of\u0026mdash;that\u0026rsquo;s just librarianship\u0026rdquo; (7). The same author goes on to say that \u0026ldquo;A community \u0026hellip; is not limited to a community of citizens in a given geographical location\u0026rdquo; before enumerating examples of communities such as schools, universities, law firms, and professional groups (Lankes, 8-9). These are examples of what I will call micro-communities, but in our hyperconnected, always online world, librarians need to consider their membership in the macro-community that is global society if they are to fulfill what Lankes calls their mission: \u0026ldquo;to improve society through facilitating knowledge creation in their communities\u0026rdquo; (17).\nOne aspect of information services librarians must consider in a global society is mobility. Libraries and librarians certainly have a responsibility to serve their immediate community, be that the geographic area that their public library serves or the university community of which their academic library is a part. But they must also serve transient members of their communities. As Lehman states, \u0026ldquo;a library can communicate and engage with visitors [emphasis mine] and city citizens in real time\u0026rdquo; (417). In other words, when considering how best to serve their communities, librarians should be aware that patrons who walk through their doors of their institution may not be permanent members of their communities, but they still have a responsibility to ensure that their access to the library\u0026rsquo;s resources is fair and equitable. This may mean being prepared to cross a language barrier or to alleviate unfamiliarity with the community in which their patrons find themselves.\nMore pressing, though, is confronting the responsibility librarians have to ensure access to the resources they steward to those whose access may be impeded by economic status, institutional or governmental policy, and other social determinants. This is especially important in the world of scholarly communication and academic libraries. Vast amounts of knowledge are created at research institutions, but those who might benefit from that knowledge may not know of its existence or, since that knowledge is often published in journals requiring expensive subscriptions to access it, may not have the means to access it due to the wrong institutional affiliation (or lack thereof). Wenzler views institutional cooperation as essential to his concept of the \u0026ldquo;collective collection\u0026rdquo; but notes that collaborating across institutional boundaries is challenging (184). Academic institutions may be competitors vying for scarce resources, so \u0026ldquo;bonds of reciprocal trust that allow groups to achieve collective ends\u0026rdquo; are harder to create (185).\nOpen Access (OA) publishing can be seen as both a way to navigate these challenges and solution that has muddied the waters. I will not discuss the practical aspects of implementing OA here, but I believe that it is based on important ideals, especially since it puts the needs of information seekers ahead of institutions. This fact seems to trouble Holley, who says \u0026ldquo;If the goal of \u0026lsquo;completely free and unrestricted access\u0026rsquo; were to be achieved, this open access would remove one of the main reasons why academic libraries exist\u0026mdash;to provide information resources to faculty and students that they could otherwise not afford\u0026rdquo; (236). Lankes\u0026rsquo;s dictum that \u0026ldquo;As \u0026hellip; communities \u0026hellip; change, so, too, must librarianship\u0026rdquo; is a helpful response here. Librarians do not exist to prolong the lives of the institutions that employ them. They exist to serve their patrons, regardless of who they are or where they come from.\nReflection # Entering this program, I assumed that I would be spending most of my time on the technical ins-and-outs of librarianship, learning cataloging, classification, and other essential elements of knowledge organization. I thought that\u0026rsquo;s what librarianship was. I am, however, so grateful that this class exposed me to the breadth of librarianship and focused especially on the human aspects of it. This class drives home the fact that librarianship is a service profession, and that those who enter it are responsible for holding themselves, their colleagues, their institutions, and their professional organizations to a high standard.\nAt first, I was skeptical that the debate surrounding whether librarians are professionals mattered; let\u0026rsquo;s just do our jobs! Now, after experiencing the thorough examination of the ALA core values this class offers, I am convinced that professionalism is essential to the practice of good librarianship.\nSome of the most challenging material in the course had to do with equity and library neutrality and objectivity. For example, when reading April Hathcock\u0026rsquo;s blog post about scholarly communication, I found myself reacting somewhat negatively to some of her arguments, particularly about how \u0026ldquo;[w]e\u0026rsquo;ve taken our diseased local system of scholarly communication and made it global\u0026rdquo; (Hathcock). I asked questions in my notes like \u0026ldquo;how is it diseased?\u0026rdquo; and \u0026ldquo;concretely, what have the west and north done wrong?\u0026rdquo; It\u0026rsquo;s not that I disagree with Hathcock. Rather, I was looking for more evidence to support her claims, and I realized that it is my responsibility to get myself up to speed on the topic and do the work necessary find that evidence.\nSimilarly, entering the program I conceived of libraries as safe, conflict-free, and neutral. In fact, that conception is part of what drew me to this program. I am delighted to have been relieved of that conception through this class. Certainly, as I stated in my earlier essay on libraries and neutrality, libraries are safe and neutral to me because I am a white man of some privilege and power. But the course material opened my eyes to the perspectives of those who are members of marginalized groups. Going forward, I feel certain that I will challenge myself and my colleagues to think critically whenever we conceive of \u0026ldquo;the public\u0026rdquo; as some monolithic group.\nAlong those lines, I think the main concepts I will carry forward with me as I finish my degree and begin my career have to do with critical cataloging. I am drawn to knowledge organization, cataloging, and classification because I am interested in how language is used to navigate the catalog and access resources available in collections. More than anything else from this course, I am grateful to be relieved of the notion that controlled vocabularies are objective. The Olson reading was eye-opening, and I feel certain that I will return to it. I pulled so many quotations from it, but I think the most important one for me to remember will be \u0026ldquo;The controlled vocabulary dictated by Cutter is, then, representation in the terms of the majority\u0026rdquo; (642). I hope to always remind myself that as an information professional, my responsibility is not just to the majority, it is to everyone.\nI am grateful for the challenge this course offered and I know that I will return to its material again and again during my degree and career.\nWorks Cited\nTwo Essential ALA Core Values\n@AletheaKontis. \u0026ldquo;YES, THIS!!! Support your local library.\u0026rdquo; Twitter, 27 Nov. 2021, 8:00 a.m.,\ntwitter.com/AletheaKontis/status/1464595100688957444.\nAlstad, Colleen and Ann Curry. (2014), \u0026ldquo;Public space, public discourse, and public libraries.\u0026rdquo;\nLibrary and Information Science Research eJournal, vol. 13. no.1, 2014. https://www.libres-ejournal.info/wp-content/uploads/2014/06/Vol13_I1_pub_space.pdf. Accessed 5 Dec. 2021.\n\u0026ldquo;Core Values of Librarianship.\u0026rdquo; American Library Association, 26 July 2006,\nhttps://www.ala.org/advocacy/intfreedom/corevalues. Accessed 5 Dec. 2021.\nMattern, Shannon. \u0026ldquo;Library as Infrastructure.\u0026rdquo; Places Journal, June 2014.\nhttps://doi.org/10.22269/140609. Accessed 5 Dec. 2021.\nOlson, Hope A. \u0026ldquo;The Power to Name: Representation in Library Catalogs.\u0026rdquo; Signs, vol. 26, no. 3,\n2001, pp. 639-668.\nTufekci, Zeynep. \u0026ldquo;Engineering the public: Big data, surveillance and computational politics.\u0026rdquo;\nFirst Monday, vol. 19, no. 7, 2014. https://firstmonday.org/ojs/index.php/fm/article/download/4901/4097. Accessed 5 Dec. 2021.\nLibraries and Neutrality\nAdler, Melissa and Lindsey M. Harper. \u0026ldquo;Race and Ethnicity in Classification Systems: Teaching\nKnowledge Organization from a Social Justice Perspective.\u0026rdquo; Library Trends, vol. 67, no. 1, 2018, pp. 52-73. Project MUSE, \u0026gt; https://doi.org/10.1353/lib.2018.0025.\nDempsey, Lorcan. \u0026ldquo;Two Metadata Directions.\u0026rdquo; LorcanDempsey.net. 21 July 2021.\nhttps://www.lorcandempsey.net/metadata-directions/. Accessed 5 Dec. 2021.\nGibson, A. N., Chancellor, R. L., Cooke, N. A., Park Dahlen, S., Lee, S. A., \u0026amp; Shorish, Y. L.\n\u0026ldquo;Libraries on the frontlines: neutrality and social justice.\u0026rdquo; Equality, Diversity and Inclusion: An International Journal, vol. 36, no. 8, 2014, pp. 751\u0026ndash;766. https://doi.org/10.1108/EDI-11-2016-0100\nSeale, Maura. \u0026ldquo;Compliant Trust: The Public Good and Democracy in the ALA\u0026rsquo;s \u0026lsquo;Core Values of\nLibrarianship.\u0026rsquo;\u0026rdquo; Library Trends, vol. 64, no. 3, 2016, pp. 585\u0026ndash;603. Project Muse, https://muse.jhu.edu/article/613925.\nInformation Services and Diverse Communities in a Global Society\nHolley, Robert P. \u0026ldquo;Open Access: Current Overview and Future Prospects.\u0026rdquo; Library Trends, vol.\n67, no. 2, 2018, pp. 214-240.\nLankes, R. David. The New Librarianship Field Guide. Cambridge, MA, The MIT Press, 2016.\nLehman, Maria Lorena. \u0026ldquo;Future-proofing the public library.\u0026rdquo; Public Library Quarterly, vol. 37,\nno. 4, 2018, pp. 408-419. https://doi.org/10.1080/01616846.2018.1513256.\nWenzler, John. \u0026ldquo;Scholarly Communication and the Dilemma of Collective Action: Why\nAcademic Journals Cost Too Much.\u0026rdquo; College and Research Libraries, vol. 78, no. 2, 2017, pp. 183-200.\nReflection\nHathcock, April. \u0026ldquo;Making the Local Global: The Colonialism of Scholarly Communication.\u0026rdquo; At\nThe Intersection, 27 Sept. 2016. https://aprilhathcock.wordpress.com/2016/09/27/making-the-local-global-the-colonialism-of-scholarly-communication. Accessed 2 Nov. 2021.\nOlson, Hope A. \u0026ldquo;The Power to Name: Representation in Library Catalogs.\u0026rdquo; Signs, vol. 26, no. 3,\n2001, pp. 639-668.\n"},{"id":10,"href":"/portfolio/docs/outcomes/articulate/rr501/","title":"LS 501: Scholarly Communication Reading Response","section":"Critically Articulating the Philosophy, Principles, and Ethics of Library and Information Science","content":"William Blackerby\nLS 501-925\nDr. Jeff Weddle\nNovember 2, 2021\nScholarly Communication Reading Response\nPrior to my engagement with the assigned readings on scholarly communication, besides knowing that peer review is important in the process, I had little idea of how scholarly journals are published. In fact, I was somewhat skeptical of why something that seemed as narrowly focused as academic publishing needed a week\u0026rsquo;s worth of our attention in this class. Thanks to these readings, though, that skepticism is gone. While I don\u0026rsquo;t have a firm or expert grasp on the economics and mechanics of scholarly publishing, I know enough now to realize that there are winners and losers in this game and the situation could (and should) be a lot better.\nBefore the Internet became the dominant system for the creation, storage, and spread of information, scholarly activity was shared in print journals stored and stewarded in academic libraries (Wenzler 183). These libraries owned the copies of the journals on their shelves and interested readers could access the information contained in them by physically entering the library. Today, with the Internet, the access picture has changed. Digital networks are, of course, a \u0026ldquo;more efficient method of sharing scholarly ideas,\u0026rdquo; but it\u0026rsquo;s also more efficient, and cost-effective, for publishers to \u0026ldquo;organize, store, and track scholarly publications\u0026rdquo; on their own digital infrastructure, removing these responsibilities from libraries in the process (Wenzler 183, 187). Nowadays, association with an institution's library grants interested readers the credentials necessary to access journal articles not by going to the library but by logging into subscription services purchased and managed by the library to read articles stored on servers controlled by the companies that publish them.\nThe Holley, Katz, and Wenzler readings make clear that a variety of labor goes into the production of scholarly communication, and this labor, of course, has associated costs. With publishers controlling nearly every aspect of this publication process, costs have grown exorbitantly. Most journal publishers are for-profit companies, and as such, their purpose, whatever the \u0026ldquo;About\u0026rdquo; page on their website may say, is to increase their profits. Wenzler tells us that \u0026ldquo;Journal prices continue to rise faster than the rate of inflation and absorb an increasing percentage of academic library budgets\u0026rdquo; (184). Because of this, \u0026ldquo;less of the library\u0026rsquo;s resources are going to staff, space, or other initiatives because more of its funding is going directly to the publishers\u0026rdquo; (Wenzler 188). Could Open Access (OA) be a solution to these costs? How hard could Open Access be to understand? Isn\u0026rsquo;t the idea that anyone can access any scholarly information they want in any quantity? What could go wrong with such an equitable, democratic vision for knowledge creation, publication, and dissemination? As it turns out, the answers to these questions are complex and involve a range of institutions, stakeholders, motivations, and power structures, especially when agreeing on just what Open Access means.\nIn addition to publishers, the major institutional players involved in scholarly communication are universities and libraries, specifically, academic libraries governed by universities. Universities employ the researchers whose scholarship fills the pages of the journals produced by the publishers. Libraries, and by extension the universities they belong to, are the publishers\u0026rsquo; customers, that is, the ones spending larger and larger portions of their budgets on the publishers' subscriptions services. Universities don\u0026rsquo;t create scholarship and libraries don\u0026rsquo;t consume it; people do. On the knowledge creation side of this arrangement, in addition to the researchers whose work is published in journals, universities also employ the scholars who serve on the editorial boards of the journals and the scholars who review the researchers\u0026rsquo; work (Doctorow). On the knowledge consumption side, library staff negotiate with publishers on prices for their subscription services so that they can create access to scholarship for their patrons. In terms of motivations, the production of research benefits universities by raising their profiles, thus augmenting key revenue streams (tuition, donations, and grants). Researchers are expected to publish in prestigious journals to advance their careers (Doctorow); Holley states that \u0026ldquo;formal peer review is an assumed requirement for tenure and promotion\u0026rdquo; and even holds universities responsible \u0026ldquo;for much of the scholarly communication crisis that led to open access because of the increasing demands upon faculty members for research publications\u0026rdquo; (220). So, not only are researchers beholden to journal publishers for the advancement of their careers, but they also often lose out even more \u0026ldquo;because most academic authors usually transfer copyright\u0026rdquo; to the publishers (Wenzler 187). What\u0026rsquo;s more, if these same authors publish in a Gold Open Access journal, they may need to pay an article processing charge since such journals are available without subscriptions (Wenzler 190, 217). As Jill Grogg, a Licensing Program Strategist with LYRASIS, said in her October 20 guest lecture on scholarly communication, \u0026ldquo;the open environment is not free.\u0026rdquo;\nAny consideration of access must also consider control. What can an individual librarian, or a small institution, control when it comes to access to scholarly information? The pessimistic answer is not much. Nevertheless, though an individual or a single institution cannot change such a convoluted system alone, they can perhaps make choices on a smaller scale that meet the information needs of their own community while advocating with others for change by those in positions of power (those with control, specifically, the publishers). This, as both Wenzler and Holley show, is much easier said than done, especially when it\u0026rsquo;s not clear what the definition of success or improvement looks like.\nAfter presenting the Budapest Open Access Initiative definition, Holley uses Suber\u0026rsquo;s definition of Open Access while noting Anderson\u0026rsquo;s comments on the implications of the definitional uncertainty surrounding the term (Holley 216). From Wenzler\u0026rsquo;s perspective, \u0026ldquo;the fundamental hurdle that prevents academic libraries from enjoying the full economic benefits enabled by digital technology is the challenge of collaborating across traditional institutional boundaries\u0026rdquo; (184). In other words, to beat the publishers at their own game, universities, who are often competitors for revenue, must cooperate. But because getting institutions to cooperate is so challenging, the present Open Access landscape is fragmented and \u0026ldquo;the power structures and elites of the publishing world have monetized an initiative that was intended to make information freely available and to reduce their power\u0026rdquo; (Holley 224).\nOf course, libraries\u0026rsquo; budgetary bottom lines are not the only aspect of Open Access that merits consideration. The point of all of this according to Cirasella is \u0026ldquo;not OA in and of itself but rather the opportunities OA presents for individuals, universities, fields of study, and global publics\u0026rdquo; (qtd. in Holley 234). As Holley says, \u0026ldquo;if the goal of open access is to provide more research for faculty and students, both green and gold [Open Access journals] have achieved some success\u0026rdquo; (235). Perhaps it is utopian or naïve, but I rather admire the Budapest Open Access Initiative definition:\nBy \u0026ldquo;open access\u0026rdquo; to this literature, we mean its free availability on the public internet, permitting any users to read, download, copy, distribute, print, search, or link to the full texts of these articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the internet itself. The only constraint on reproduction and distribution, and the only role for copyright in this domain, should be to give authors control over the integrity of their work and the right to be properly acknowledged and cited. (qtd. in Holley 216).\nHolley, on the other hand does not, saying\nIf the goal of \u0026ldquo;completely free and unrestricted access\u0026rdquo; were to be achieved, this open access would remove one of the main reasons why academic libraries exist\u0026mdash;to provide information resources to faculty and students that they could otherwise not afford. While faculty and students might still need help in navigating the new structures, this free availability would lessen their dependence on the digital library in the same way that digital resources have reduced the need to come to the physical library. (236)\nI certainly hope Holley does not think that the purpose of libraries is to get people to come to the library. I can\u0026rsquo;t imagine a world in which information seekers don\u0026rsquo;t need the assistance of information professionals for help navigating available resources, no matter the form they take or the means of publication. Libraries are about getting information to those who need it, not getting people in the door or on the website.\nI save my discussion of Hathcock for last because in some ways it seems the most important of the readings for this topic and is the one I struggled with the most. Hathcock\u0026rsquo;s blog post was a helpful reminder of the human side of all of this, far more so than anything we read that was published in a scholarly journal. It challenged me and left me with questions that I almost feel silly asking and that point to my need to educate myself about colonialism and neoliberalism. In my reading notes on the piece, I ask \u0026ldquo;concretely, what have the west and the north done wrong?\u0026rdquo; The high school history answers to this are self-evident, but I admit I have work to do when it comes to learning how to answer these questions for myself through the lens of scholarly communication and knowledge creation in the present day. As soon as I found myself wishing for more on these topics in Hathcock\u0026rsquo;s piece, I realized her admonition to \u0026ldquo;Look it up!\u0026rdquo; and the blog post it linked to were speaking directly to me.\nThat I have barely scratched the surface in discussing the complexity of scholarly communication in the present day not only reflects my surface level understanding of it, but also the fact that the situation is vast, ever-changing, and requires a certain level of comfort with economics. But the basic facts are easy to understand: the publishers run the show and researchers and information professionals want the situation to be better, but because cooperation across institutional boundaries is hard, Open Access solutions are fragmented and sometimes coopted by the publishers. Unlike at least one of the authors we read, I am new enough to this to still hold out hope that genuinely open and inclusive scholarly communication can become a reality.\nWorks Cited\nDoctorow, Cory. \u0026ldquo;All of science gets a general index.\u0026rdquo; Pluralistic: Daily links from Cory\nDoctorow, 28 Oct. 2021. https://pluralistic.net/2021/10/28/clintons-ghost/#cornucopia-concordance. Accessed 2 Nov. 2021.\nGrogg, Jill. \u0026ldquo;Scholarly Communication.\u0026rdquo; LS 501, University of Alabama. Received 20 Oct.\n2021. Course Guest Lecture.\nHathcock, April. \u0026ldquo;Look It Up *Wavy Hand Emoji*.\u0026rdquo; At The Intersection, 18 Aug. 2016.\nhttps://aprilhathcock.wordpress.com/2016/08/18/look-it-up-wavy-hand-emoji. Accessed 2 Nov. 2021.\nHathcock, April. \u0026ldquo;Making the Local Global: The Colonialism of Scholarly Communication.\u0026rdquo; At\nThe Intersection, 27 Sept. 2016. https://aprilhathcock.wordpress.com/2016/09/27/making-the-local-global-the-colonialism-of-scholarly-communication. Accessed 2 Nov. 2021.\nHolley, Robert P. \u0026ldquo;Open Access: Current Overview and Future Prospects.\u0026rdquo; Library Trends, vol.\n67, no. 2, 2018, pp. 214-240.\nWenzler, John. \u0026ldquo;Scholarly Communication and the Dilemma of Collective Action: Why\nAcademic Journals Cost Too Much.\u0026rdquo; College and Research Libraries, vol. 78, no. 2, 2017, pp. 183-200.\n"},{"id":11,"href":"/portfolio/docs/outcomes/practice/discussion-questions/","title":"LS 506: Ethical Cataloging Discussion Questions","section":"Practicing Principles of Social and Cultural Justice","content":" Describe ethical concerns that occur in cataloging work. # In their exploration of what ethical cataloging is, Snow and Shoemaker (2020) note Intner's framing of cataloging ethics ``largely in terms of of institutional responsibilities: the library's ability or inability to meet its users' needs'' ( Citation: Snow\u0026#32;\u0026amp;\u0026#32;Shoemaker,\u0026#32;2020,\u0026#32;p.\u0026nbsp;535 Snow,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;Shoemaker,\u0026#32; B. \u0026#32; (2020). \u0026#32;Defining Cataloging Ethics: Practitioner Perspectives. Cataloging \u0026amp; Classification Quarterly,\u0026#32;58(6).\u0026#32;533–546. https://doi.org/10.1080/01639374.2020.1795767 ) . In other words, ethical cataloging is about how the library catalog affects the people who use it.\nEthical concerns in cataloging work center on the authorized terms used in controlled vocabularies that are applied to resources in the collections of libraries, archives, and other information organizations. In particular, these ethical concerns arise around terms used to describe human identity. Specific examples include terms applied to gender identity and expression, sexual orientation and identity, race, ethnicity, and immigration status.\nTwo of this week's resources provide concrete examples of the ethical concerns mentioned above. Billey, Drabinski, and Roberto (2014) apply queer theory to the concept of gender in the catalog in their critique of RDA 9.7. The film Change the Subject details the experiences of undocumented students at Dartmouth College after encountering the formerly authorized Library of Congress Subject Heading ``illegal aliens'' and their efforts to change the heading. Responses to later questions will discuss these ethical concerns in more detail.\nHow can information found in name and subject authority records be offensive to people? # One of the main ways information found in authority records can be offensive to people is through misrepresentation. ``Misrepresentation,'' according to Smiraglia (2009), ``or biased representation, occurs when a resource is described in terms that are not consistent with the uses it might engender in a particular milieu'' [@smiragliaBibliocentrismCulturalWarrant2009, p. 675]. Change the Subject provides a powerful, concrete example of this.\nMelissa Padilla, a Dartmouth undergraduate at the time, encountered the authorized Library of Congress Subject Heading ``illegal aliens'' while seeking help from Jill Baron, a Dartmouth librarian, in developing an independent study (Broadley et al., 2019). Padilla's reaction is striking:\nMy gut reaction to seeing the subject heading, here at Dartmouth, was disgust, and also like perplexed, I was like why? Like, I thought this place would know better or do better. Wow, I can't believe you think these things. I know that there's undocumented students here on campus and you recruit them, and you still subject them to this sort of thing, and it's not okay (Broadley et al., 2019, 5:06).\nThrough this experience Padilla and other members of CoFIRED, the Dartmouth Coalition for Immigration, Reform, Equality and DREAMers learned that the offensive term wasn't only in use at Dartmouth, but was in fact part of a national standard (Broadley et al., 2019, 6:29, 21:58). In other words, this was problematic at a national level, and more people than just Dartmouth undergraduates were being affected.\nHow can content standards (e.g., RDA, AACR2) in cataloging be ethically problematic for users of the catalog? # Billey, Drabinkski, and Roberto (2014) argue against the notion that the gender marker in RDA 9.7 is ``an objective description of reality'' and is in fact ``passively harmful to transgender individuals'' [@billeyWhatGenderGot2014, p. 414]. RDA, in their analysis, does this by denying the existence of genders that fall outside the male-female binary. In other words, library users who identify outside the gender binary will not see themselves reflected in the catalog.\nThe same authors note that catalogers, in the quest to provide complete records, have the leeway ``to codify erroneous information'' [@billeyWhatGenderGot2014, p. 418]. They give the example of New Orleans bounce artist Big Freedia, who, according to a 2011 interview, ``fluidly use[s] masculine and feminine pronouns'' but identifies as a gay man [@billeyWhatGenderGot2014, p. 418]. But the cataloger who created the authority record for Big Freedia made an erroneous assumption about the artist's gender: ``The authority record for the gender-bending hip-hop artist, Big Freedia records his gender as `female''' [@billeyWhatGenderGot2014, p. 418].\nReturning to the discussion of the offensive former LCSH term for undocumented immigrants, in the film Change the Subject, Dartmouth librarian Jill Baron notes how neutrality can in fact be harmful: ``I didn't look at the system, the cataloging system as actually an expression of values. I just sort of saw it as this disassociated, neutral, organizing principle'' (Broadley et al., 2019, 9:59). In the same film, Dr. Claudia Anguiano Evans-Zepeda, a former faculty advisor to the Dartmouth student group CoFIRED, succinctly states exactly why the former subject heading is so offensive: ``It's wielded most specifically towards migrants of color and there's a racist coding to it and by virtue of its power it becomes part of the ways in which anti-immigrant sentiment festers'' (Broadley et al., 2019, 11:29).\nGiven that cataloging systems are biased, how then do we organize and describe information ethically? # To organize and describe information ethically, we begin by recognizing that just as the perfect metadata record does not exist, the perfectly ethical metadata record does not exist either. Cataloging is never complete; it is an ongoing process, and attention to changing cultural norms is a part of this process.\nOne specific way to organize and describe information ethically is to question standards, which Snow and Shoemaker (2020) define as ``not strictly following standards such as RDA, LCSH, but modifying them at the local level or attempting to change the standard to better serve users and/or be more respectful of others; recognizing that standards are biased'' [@snowDefiningCatalogingEthics2020, p. 538].\nThe same authors argue for working from shared values in the field of cataloging; they enumerate five such values synthesized from responses to a questionnaire they shared with catalogers:\nAccessibility of resources and metadata Awareness of personal, institutional, and standards bias Inclusive metadata and actions Accurate representation of resources and agents Adhering to standards while interrogating their usefulness ( Citation: Snow\u0026#32;\u0026amp;\u0026#32;Shoemaker,\u0026#32;2020,\u0026#32;p.\u0026nbsp;543 Snow,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;Shoemaker,\u0026#32; B. \u0026#32; (2020). \u0026#32;Defining Cataloging Ethics: Practitioner Perspectives. Cataloging \u0026amp; Classification Quarterly,\u0026#32;58(6).\u0026#32;533–546. https://doi.org/10.1080/01639374.2020.1795767 ) Most important, though, is to listen to feedback shared by library patrons, stakeholders, and other constituents, especially those from marginalized communities, in the ongoing effort to prevent the library catalog from causing harm.\nReferences # Snow, K., \u0026amp; Shoemaker B. (2020). Defining cataloging ethics: practitioner perspectives. Cataloging \u0026amp; Classification Quarterly, 58(6), 533-546. https://doi.org/10.1080/01639374.2020.1795767\nSmiraglia, R. P. (2009). Bibliocentrism, cultural warrant, and the ethics of resource description: A case study, Cataloging \u0026amp; Classification Quarterly, 47(7), 671-686. https://doi.org/10.1080/01639370903112013\nBilley, A., Drabinski, E., \u0026amp; Roberto, K. R. (2014). What's gender got to do with it? A critique of RDA 9.7. Cataloging \u0026amp; Classification Quarterly, 52(4), 412-421. https://doi.org/10.1080/01639374.2014.882465\nBroadley, S., Baron, J., Cornejo Cásares, Ó.S., and Padilla, M. (2019). Change the subject [Video]. Dartmouth Digital Collections. https://n2t.net/ark:/83024/d4hq3s42r\nBilley,\u0026#32; Drabinski\u0026#32;\u0026amp;\u0026#32;Roberto (2014) Billey,\u0026#32; A.,\u0026#32; Drabinski,\u0026#32; E.\u0026#32;\u0026amp;\u0026#32;Roberto,\u0026#32; K. \u0026#32; (2014). \u0026#32;What’s Gender Got to Do with It? A Critique of RDA 9.7. Cataloging \u0026amp; Classification Quarterly,\u0026#32;52(4).\u0026#32;412–421. https://doi.org/10.1080/01639374.2014.882465 (N.A.) (n.d.) (N.A.). \u0026#32; (n.d.). \u0026#32; Change The Subject. \u0026#32;Retrieved from\u0026#32; https://dartmouth.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=cc863047-6c5d-4751-a201-adc30026842d Smiraglia (2009) Smiraglia,\u0026#32; R. \u0026#32; (2009). \u0026#32;Bibliocentrism, Cultural Warrant, and the Ethics of Resource Description: A Case Study. Cataloging \u0026amp; Classification Quarterly,\u0026#32;47(7).\u0026#32;671–686. https://doi.org/10.1080/01639370903112013 Snow\u0026#32;\u0026amp;\u0026#32;Shoemaker (2020) Snow,\u0026#32; K.\u0026#32;\u0026amp;\u0026#32;Shoemaker,\u0026#32; B. \u0026#32; (2020). \u0026#32;Defining Cataloging Ethics: Practitioner Perspectives. Cataloging \u0026amp; Classification Quarterly,\u0026#32;58(6).\u0026#32;533–546. https://doi.org/10.1080/01639374.2020.1795767 Congress (n.d.) Congress,\u0026#32; T. (s.d.).\u0026#32;Retrieved from\u0026#32; https://id.loc.gov/authorities/subjects/sh85003545.html "},{"id":12,"href":"/portfolio/docs/outcomes/use/discussion-questions/","title":"LS 506: Metadata Quality Discussion Questions","section":"Using Evidence to Inform Library and Information Practices","content":"\\usepackage{geometry}{=latex}\n\\geometry{margin=1in}{=latex}\n\\usepackage{setspace}{=latex}\n\\doublespacing{=latex}\n#+cite_export: biblatex apa Discuss the different ways we can assess the quality of cataloging. # Any discussion of the quality of cataloging needs to begin with a definition of quality cataloging. Snow states that\nIn cataloging literature, \u0026ldquo;quality cataloging\u0026rdquo; has been defined in the following ways:\nAccurate bibliographic information that meets users\u0026rsquo; needs and provides appropriate access in a timely fashion. What library users say it is. Level of content (AACR2 level of description, inclusion of subject classification or subject headings, authority control of headings, etc.)\u0026hellip;accuracy of content (in transcription from the item, in conformity with the standards applied)\u0026hellip;fitness for purpose. We define quality for support staff by percentage error rate in the following: selection or suitability of OCLC record as a match for item cataloged; correcting typographical errors in the following fields: 100, 245, 260, 300, 5xx; making appropriate edits to bibliographic and holdings records; accurate creation of item and holdings records; recognizing cataloging problems and bringing them to the attention of a supervisor. For cataloging librarians: quality is defined by excellent original cataloging based on AACR2 full-level standards; name authority records created to standards set by NACO; effective supervision of support staff, including timely resolution of questions and problems; a reasonable turnaround time for materials so that a backlog is not created or growing; responsiveness to needs of internal and external patrons; completeness, efficiency, responsive to queries and complaints. [@snowDefiningAssessingRethinking2017, pp. 438-439]\nKeeping in mind that data quality and quality cataloging are separate but perhaps overlapping concepts, the definition of data quality presented in Badovinac (2021) in the context of evaluating quality of cataloging in the Slovenian union bibliographic database is applicable to multiple contexts:\nQuality data is present in a unique bibliographic or authority record and it is not redundant. It is structurally consistent, structurally complete, semantically accurate, syntactically accurate, value complete, semantically coherent, representationally consistent, current, and may have added value. [@badovinacDefiningDataQuality2021,\np. 366]\nUltimately, though, there is no one size fits all definition, so quality cataloging needs to be defined based on institutional context and institutional priorities or goals. Perhaps the best way to evaluate quality of cataloging is to respond to the following questions presented in Bade (2008)\nWhat data elements are useful for the kind of library research performed here in this particular institution? How much, and which elements of that necessary information can this institution afford to support? (This means either creating it initially, correcting or adding it to bibliographic records imported from external sources, and future maintenance in cases of changing standards, new headings, data definitions, etc.) [@badePerfectBibliographicRecord2008, p. 129]. How often should we be evaluating our cataloging if quality is always changing? # Badovinac (2021) presents several methods used for assessing and improving data quality in the Slovenian union bibliographic database, including ``regular reviews of records that were made by beginning catalogers, identification of record groups with common error patterns, yearly reviews of 100 randomly selected records, and continuous monitoring of the recently created bibliographic records and authority records'' [@badovinacDefiningDataQuality2021, p. 361]. Badovinac goes on to provide more detail on the continuous monitoring of bibliographic and authority records, also known as daily production monitoring, stating that ``[i]ts primary goal is to correct and resolve the errors in recently created records'' [@badovinacDefiningDataQuality2021, p. 361]. The monitoring method is conducted by random sampling of metadata; records are reviewed without a resource in hand, then recommendations are made to catalogers who can correct their records with a resource in hand. The process is completed within six days after creation and sampling of records [@badovinacDefiningDataQuality2021, p. 361].\nIn addition to providing a practical answer to the previous question of how to assess quality of cataloging, Badovinac gives a detailed answer to the question of how frequently cataloging should be evaluated. The short, but broad, answer is ``continuously.'' A practical, effective definition of ``continuously'' is going to depend on institutional goals and capacity constraints, specifically constraints imposed by finances and labor availability.\nShould the quality of cataloging be determined by the local needs of a library or by the broader cataloging community? # Standards are essential to quality cataloging, but the local needs of a library should have the final say in assessing quality of cataloging. Snow says it best: ``While standardization of bibliographic data remains important (especially now since machines need to read and act upon this data on the web), standardization without concern for user needs does not produce quality cataloging'' [@snowDefiningAssessingRethinking2017, p. 450]. Snow further states that ``the developers of cataloging standards have largely neglected to study users as part of the development process and therefore current standards may not accurately reflect the needs of modern library users'' [@snowDefiningAssessingRethinking2017, p. 447]. Every library user is different, and every library is different, so libraries should follow Snow's recommendation to ``[c]onduct a study of user information needs to determine what information is important to users at [their] institution and try to accommodate those needs in a way that is financially feasible, as suggested by Bade'' [@snowDefiningAssessingRethinking2017, p. 451].\nHow do we balance the needs of library users and catalogers when it comes to improving the quality of cataloging? # The balance needs to be tilted in favor library users, and catalogers should recognize that their work is primarily for library users. As Bade says, ``[u]sers of the library do not need bibliographic records at all, perfect or not. What they want is to find what they are looking for'' [@badePerfectBibliographicRecord2008, p. 125]. That said, Flynn and Kilkenny illustrate how poor quality records can impede the user experience and the cataloger's ability to improve the user experience. For example, ``[v]endor provided MARC records can vary in quality which affects the amount of time needed for cataloging'' [@flynnCatalogingCenterImproving2017, p. 632]. This can lead to backlogs which limit findability of items in a library's collections. Flynn and Kilkenny explain that institutions like Duke University have policies in place that reduce cataloging to core needs in order to increase staff productivity [@flynnCatalogingCenterImproving2017, p. 632]. OhioLINK, on the other hand, ``does full-level cataloging, ensuring that call numbers and subject headings are included and that descriptive metadata is accurate, especially the title field and author(s)/editor(s)'' [@flynnCatalogingCenterImproving2017, p. 635]. Furthermore, record maintenance projects in the OhioLINK consortium ``prioritize user access as well as record standardization and consistency to ensure all e-book records meet current consortial cataloging standards'' [@flynnCatalogingCenterImproving2017, p. 638].\n"},{"id":13,"href":"/portfolio/docs/outcomes/use/memo/","title":"LS 535: Case Study Memorandum","section":"Using Evidence to Inform Library and Information Practices","content":" +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ | # Indian Springs School | +=======================================================================+ +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+ First table has company name and second table has memo information\nTo: Tanya Yeager, Assistant Head of School for Finance and Administration\nFrom: William Blackerby, Records Management Consultant\nCC: Scott Schamberger, Head of School\nDate: August 2, 2022\nRe: Implementing a Records Retention Plan\n: First table has company name and second table has memo information\nAs the National Association of Independent Schools Legal Advisory titled \u0026ldquo;Records Retention: What, How Long, and How?\u0026rdquo;, which was last updated in June of 2018, states, \u0026ldquo;[s]chools are record intensive places\u0026rdquo; (p. 21). Indian Springs School is no exception. The school handles academic records, health records, athletic records, employment records, board meeting minutes, donation records, and the list goes on. Unfortunately, while individual departments may or may not have ad hoc plans for managing their own records, there is no comprehensive plan for the management and retention of these records. This memorandum calls for the creation and implementation of an integrated, overarching records management and retention system for Indian Springs School based on the NAIS legal advisory.\nThe Why of Records Retention\nDebra P. Wilson and Whitney Silverman, the authors of the aforementioned whitepaper, state that \u0026ldquo;Proper document retention is necessary in the event of a lawsuit, helps a school capture intellectual property in terms of courses and curriculum, and makes it easy for a school to know with reasonable certainty that it has all the documents it needs for any purpose\u0026rdquo; (p. 1). In other words, records retention helps an institution know what it knows. Besides the regulatory aspects of records retention, schools \u0026ldquo;should also be driven by business purpose and usefulness\u0026rdquo; (p. 2). Sound records retention makes informational queries easier to respond to. Of particular import for our context is the ability to respond to internal informational queries.\nElectronic Records\nIndian Springs School juggles multiple platforms for electronic records management. The main tool used across all departments is Google Workspace. Because of the generous storage capabilities provided by this platform, it easy, as Wilson and Silverman say, \u0026ldquo;to store vast amounts of information \u0026ndash; which also makes it harder [to manage records] since there can be so many more records to sort through\u0026rdquo; (p. 2). This is evidenced by the vast numbers of unorganized digital files shared across Indian Springs School\u0026rsquo;s Google Workspace. Generally speaking, these files lack a taxonomic structure shared across the school, so locating a desired record requires using the tool\u0026rsquo;s search function to craft an effective query, which can be a trial-and-error process.\nFederal Record Retention Requirements\nAn essential element of records retention is being prepared for legal action. The following is a summary of federal actions that schools should be prepared for drawn from the Wilson and Silverman whitepaper (pp. 4-7).\nAmerican with Disabilities Act (ADA)\nEmployment Records: One Year\nEmployment Discrimination Filing: Until Matter is Resolved\nPublic Accommodations Records: No Set Time\nFair Labor Standards Act (FLSA)\nPayroll: At least three years\nRecords for Wage Computations: Two Years\nFamily Medical Leave Act (FMLA)\nRecords related to FMLA Leave: Three Years I-9 Forms\nForms: Three Years after Hiring of One Year after End of Relationship, Whichever is Later Employment Retirement Income Security Act (ERISA)\nVaries depending on specific benefits offered Tax\nEmployment Tax Records: Four Years The ADA merits additional note. According to Wilson and Silverman, two sections are relevant to independent schools like ours: \u0026ldquo;Title deals with employment matters and Title III applies to students and other matters of public accommodation\u0026rdquo; (p. 4). As the number students with accommodations we enroll increases, we must ensure that we are in compliance with Title III of the ADA.\nState Record Retention Requirements\nIn addition to federal law, the school is subject to state records management laws as well. The relationship between federal and state requirements is as follows: \u0026ldquo;states cannot lessen a federal requirement, but they may extend it. Also, states may have regulations where the federal government has none\u0026rdquo; (Wilson and Silverman, p. 7).\nThese records can be broken down into permanent and temporary records. Examples of permanent records in schools are \u0026ldquo;basic identifying information, academic transcripts, and attendance records\u0026rdquo; (p. 8). A decision for the school to make would be whether or not to include \u0026ldquo;long-term suspensions, expulsions, and health records\u0026rdquo; in permanent records (p. 8). The school\u0026rsquo;s legal counsel should be consulted on this matter.\nTemporary records may need to be kept for three to six years; such records may include family background information, extracurricular records, teacher notes, disciplinary information, psychological information, and health information (p. 8). Legal counsel should also be consulted on this matter to ensure the school is keeping the appropriate records.\nFinally, counsel should also be consulted about state requirements for immunization records.\nStatutes of Limitations\nThis memorandum does not address statutes of limitations in depth or detail, but such statutes should be taken into consideration during the process of creating the records retention system, under guidance of school counsel as usual. Counsel should especially be consulted about records related to sexual abuse claims.\nDocument Destruction\nAs part of the process of developing a records retention system, \u0026ldquo;a point person who is aware of all document destruction should be\u0026rdquo; identified (p. 10). The decision should also be made about whether document destruction will be conducted in-house or by an outside vendor. Additionally, the length of time electronic records are retained should be decided on as well.\nA Records Retention System Development Plan for Indian Springs School\nThis section of the memorandum draws from and elaborates on the guidelines established in the NAIS whitepaper on how to create a records retention system (pp. 12-14).\nThe Document Team\nComprised of IT director, Assistant Head of School for Finance and Administration, Assistant Head of School for Academics, Assistant Head of School for External Affairs, Dean of Faculty, Dean of Students, School Nurse, School Counselor, Athletic Director, Director of College Advising, Director of Enrollment Management, School Archivist\nResponsible for developing the overall records retention plan for the school\nSchool Counsel\nThe Assistant Head of School for Finance and Administration will contact the school\u0026rsquo;s attorney to request a brief on:\nStatues of limitations in Alabama for records in the school\u0026rsquo;s care\nStudent records requirements for the State of Alabama\nAdvice on crafting a document retention plan\nAuditor\nThe Assistant Head of School for Finance and Administration will contact the school\u0026rsquo;s auditing firm, Dent Moses, to solicit input on document retention with an eye towards making the annual audit a smoother process. Goals\nThe goals of this plan are to\nMigrate as many records as possible to an internal electronic database\nDestroy records that are no longer necessary, with input from the School Archivist\nCreate and implement an effective records retention schedule.\nDocuments\nEach member of the Document Team should come to the initial meeting with a comprehensive list of the documents each department they represent create, specifying whether they are print or electronic and how long they are currently kept. This will serve as a sort of \u0026ldquo;rough draft\u0026rdquo; of the final retention schedule. Point Person\nThe School Archivist will be the point person for records management. The School Archivist will report to the Assistant Head of School for Finance and Operations through the school\u0026rsquo;s business office. Training\nThis is the key element of the whole plan. The Document Team will need to develop a plan to train staff. For example:\nTeachers\nShould be trained on how long to keep communications with students and parents/guardians Supervisors (department chairs, deans, directors)\nShould be trained on how long to keep emails, memoranda, and other communications sent to their direct reports Administrative Staff (receptionist, communications staff)\nShould be trained on where and how to store official school communications, e.g., emergency messages, letters from the Head of School\u0026rsquo;s office and various deans' offices. Review of Retention Schedule\nThe Document Team should review the retention schedule every three years. Sample Retention Schedule\nBelow is a sample Records Retention Schedule drawn from the NAIS legal advisory (p. 16). This should be distributed to the Document Team to help them visualize what the final Indian Springs School specific retention schedule might look like and to give members of the Document Team a starting point for the listing the types of documents they are responsible for.\n{width=\u0026ldquo;6.5in\u0026rdquo; height=\u0026ldquo;7.659722222222222in\u0026rdquo;}\nIn addition to consulting with school counsel and the school\u0026rsquo;s auditor, the school should take advantage of the records management policy shared by the Ravenscroft School and outlined below.\nOutline of Ravenscroft School Document Retention Policy, to be used as a model\nI. Objectives\na. This section establishes the goals of the policy. II. Policy\na. This section enumerates the general records retention policy. III. Responsibility\na. This section establishes the responsibility each school employee has to steward the documents in their care. IV. Definitions\na. This section defines records management related terms and roles that will be used throughout the document. V. Administration\na. This section lists the individuals responsible for records management and specifies their roles. VI. Implementation\na. This section enumerates in detail the operational procedures for executing the Document Retention Policy VII. Amendment Procedure\na. This section explains the proper procedure for amending the Document Retention Policy and the Records Retention Schedule. Conclusion\nThis memorandum has served as a summary of the NAIS legal advisory on records management in independent schools as well as an outline of how its recommendations can be applied to Indian Springs School.\nMy recommendation is that the school form the Document Team in August of this year with a goal of finalizing a schoolwide records retention policy, including a records retention schedule, by December 31.\nThough engaging in such a project will no doubt be time and labor intensive for school leaders, it should lead to improvements in efficiency across the school.\n"},{"id":14,"href":"/portfolio/docs/outcomes/evaluate/notes/","title":"LS 562: Digital Library Collection Development Policy Analysis and Services Description","section":"Evaluating Technology-Mediated Access in Library and Information Services","content":"\\usepackage{geometry}{=latex}\n\\geometry{margin=1in}{=latex}\n\\usepackage{setspace}{=latex}\n\\doublespacing{=latex}\n#+cite_export: biblatex apa Introduction # According to their website,\nThe digital collection services of the Williams College Special Collections seeks to collect, manage, preserve and disseminate digital objects that support the educational mission of the College, are in need of preservation, are part of the history of the College, and are rare and unique. [@DigitalCollections, para. 1]\nIn this first part of this paper I will describe and analyze the Williams College Standards, Policies and Procedures for Digital Collections the using the framework presented in Xie and Matusiak's Discover Digital Libraries: Theory and Practice [@xieDiscoverDigitalLibraries2016b]. As part of this analysis and description, I will investigate the extent to which the Williams College digital library collection development policies take project costs and project funding into account.\nIn the second part of this paper, I will enumerate and describe digital services offered by Williams College Special Collections.\nDigital Collection Development Policy Analysis # Xie and Matusiak present the California Digital Library (CDL) collection development framework as an example of a digital library collection development policy [@xieDiscoverDigitalLibraries2016b, p. 39].\nThe California Digital Library (CDL) is a ``co-library'' of the University of California whose primary collection responsibility is to develop electronic content and make it available to all faculty and students. Some of the electronic content will be licensed and acquired from commercial sources, and some will be produced by digitizing University collections. The same three considerations used to develop library collections in the ten UC campus libraries guide collection development for the CDL:\nThe user base The programs that are to be supported The resources available to support those users and programs [@CollectionDevelopmentFramework, para. 1] Below, I present an analysis of Williams College Standards, Policies, and Procedures for Digital Collections using the CDL framework.\nUser Base # Given that the Williams College Special Collections, of which the Digital Collections are a part, are housed at an academic institution, it is safe to assume that the primary user base are the faculty and students of Williams College. This is not explicitly stated within the Collection Development policies, but can be inferred from other Special Collections policies. For example, the Access and Use Policies state that items ``in Special Collections are acquired and preserved in order to support research in all areas of the Williams curriculum'' [@AccessUsePoliciesa, para. 1]. Additionally, class visits to Williams College Special Collections are available for ``facilitating student engagement with primary sources and object-based learning'' [@ClassVisits, para. 1]. Special Collections staff is also ``happy to help select material and work with faculty to structure a class'' [@ClassVisits, para. 2]. Librarians and archivists are also available for one-on-one appointments to assist with research projects [@ResearchAppointments, para. 1].\nThough it is clear that the primary user base of the Williams College Special Collections is the College's faculty and students, the Access and Use Policies clearly state that ``[y]ou do not need to be affiliated with Williams in order to use our collections'' [@AccessUsePoliciesa, para. 1].\nSupported Programs # There is no explicit enumeration of programs supported by the Williams College digital collection services in the policy documents, but they do state that ``[t]he development of digital collections is in keeping with Special Collections' intention to describe, arrange, digitize, and disseminate our collections and to provide `the retention, preservation, and research use of its collections''' [@DigitalCollections, para. 2]. More specifically, ``[t]he focus of the Williams College's [sic] Digital Collections is unique or rare content from collections owned by Special Collections or the College at large'' [@DigitalCollections, Scope]. In other words, the programs supported by the Williams College digital collection services are the programs of Williams College Special Collections more broadly.\nResources Available # The Digital Collections policy document states that\nDigital Collections are composed of digital objects, whether born-digital or digitized, regardless of item type. Text, audio, still and moving images, datasets, etc. are all included. Collecting activities focus on content that can be made accessible to a wide audience [@DigitalCollections, para. 3].\nWithin the scope of Williams College Digital Collections are ``other types of open access scholarly materials (for example, data sets, data visualizations, creative works, working papers, preprints, publications, etc.)'' [@DigitalCollections, Scope]. ``Unique and rare content'' from Special Collections may be made available ``through digitization of selected analog materials or through the transfer of born-digital content'' [@DigitalCollections, para. 3].\nFurthermore,\nSpecial Collections is highly supportive of collections that:\nRepresent the cultural, geographic, economic, and political diversity of Williams College. Constitute objects for which access would be improved by inclusion in Special Collections: linking between items, innovative ways to comprehend the material, etc. May be difficult to access physically. Comprise materials that are in need of preservation. Complement existing digital content. Have materials that are dispersed. [@DigitalCollections, Scope] Digitization of ``non-unique images for teaching'' falls outside the scope of the Digital Collections policy document [@DigitalCollections, Scope].\nProject Costs and Funding # Capacity Constraint # Weber et al. define collection management as ``[a]ll of the activities that are necessary to ensure that collection material is described, discoverable, and available for use,'' including ``accessioning and processing, cataloging, conservation, and digitization,'' the latter of which is of particular interest in this paper [@weberTotalCostStewardship2021, p. 2]. Weber et al. also move beyond budgetary constraints on collecting and by introducing the capacity constraint model into collection development. They define capacity constraint broadly as ``[f]actors that limit production, performance, or output'' and specifically in the context of collection development they say that ``a capacity constraint impacts an institution's ability to accomplish collection management activities'' [@weberTotalCostStewardship2021, p. 3].\nCollaborative Decision Making # Weber et al. state that ``[i]nformed decision-making around collections requires that everyone involved in the lifecycle of collection stewardship has a shared understanding of when and how various decisions are made, and by whom'' [@weberTotalCostStewardship2021, p. 13]. Furthermore, ``[e]nsuring that information is exchanged when and as needed is typically easier said than done, especially in large organizations and over extended periods of time'' [@weberTotalCostStewardship2021, p. 13].\nCapacity Constraint and Collaborative Decision Making at Williams College Special Collections # Though there is no comprehensive discussion of project costs and funding, the digital collection development policy document does note that some digitization activities fall into the category of ``[s]pecially-funded digitization projects made possible through grants or gifts'' and ``may involve additional short-term staff and will likely be subject to specific deadlines and special project parameters'' [@DigitalCollections, Digitization]. This can be interpreted as an increase in capacity in the short term for specific projects. There is also an endowed fund, the John T. Gibson '35 Williamsiana Fund, for the support of Special Collections [@GivingCollegeArchives].\nThe Digital Collections services of Special collections does ``not digitize or accept digitized versions of inactive analog records of enduring value that have not been officially transferred to the Special Collections, though Special Collections reserves the right to make exceptions if warranted'' [@DigitalCollections, Scope]. This can be read as a capacity-conserving decision. In other words, digitization is not an endlessly available service, and certain channels must be followed by those who want to engaged the service.\nThere is also no comprehensive discussion of collaborative decision making in the reviewed policy documents. Perhaps this is because Williams College Library and Special Collections are a small organization given that Williams is a liberal arts college and not a large research university. Nevertheless, the Digital Collections policy document does note that decisions regarding the removal of materials ``will be made in conjunction with the Digital Resources Archivist/Records Manager, Archivist, Director of Libraries, and others as appropriate and will be handled on a case by case basis'' [@DigitalCollections, Maintenance and removal]. This is in line with Weber et al.'s discussion of collaborative decision making.\nDigital Library Services Description # Legal/Copyright Services # Williams College Library and Special Collections do not provide any legal or copyright services, per se, but they do invite patrons to ``use materials in the public domain and to make fair use of copyrighted materials as defined by copyright law'' [@AccessUsePoliciesa, Copyright and permissions]. Items ``in the public domain can be added to a collection without obtaining permission from anyone'' [@xieDiscoverDigitalLibraries2016b, p. 44]. Fair use ``is a powerful tool for using [copyrighted] works without obtaining permission'' [@xieDiscoverDigitalLibraries2016b, p. 47].\nSmith (2012) outlines approaches libraries can take to protect themselves from potential litigation regarding copyright violations [@smithCopyrightRiskManagement2012]. Williams College Special Collections emphasizes the users' responsibility for ``determining whether [their] use is fair and for responding to any claims that may arise from [their] use'' [@AccessUsePoliciesa, Copyright and permissions]. Regarding digital collections, specifically, Special Collections ``may need to restrict access to the College\u0026rsquo;s records in order to adhere to copyright laws or restriction periods'' [@DigitalCollections, para. 3]. Special Collections encourages items and collections that ``[h]ave copyright clearance if the copyright is not held by the submitter/s or Williams College'' [@DigitalCollections, Scope]. See the section on ``Maintenance and Removal'' below for more information on how Williams College Special Collections handles copyright infringement and disputes.\nPersonal Archiving # Williams College Special Collections ``encourage[s] potential contributors interested in digitizing their own content in preparation to be added to Special Collections, to coordinate with the Digital Resources Archivist/Records Manager in advance to establish the correct digitization methods'' [@DigitalCollections, Personal archiving].\nAssociated Analog Material # Williams College Special Collections requires that ``inactive records of enduring value\u0026hellip;be transferred to Special Collections before that content can be added to the digital collections'' [@DigitalCollections, Associated analog material]. This allows ``staff to have continued access to analog original materials in case re-digitization is ever necessary'' [@DigitalCollections, Associated analog material].\nDigitization Services # ``Digitization is the process of creating digital representations of information resources recorded on analog carriers'' [@xieDiscoverDigitalLibraries2016b, p. 59]. The Williams College Digital Collections policy document identifies three categories digitization work is likely to fall into.\nOngoing digitization of whole (or large portions of) collections. This work is not subject to specific deadlines. Project-specific digitization work, typically tied to grant-funding or gifts. Scan on Demand, or digitization that is based on user request of rare or unique materials. [@DigitalCollections, Digitization]\nMetadata Services # Williams College Special Collections collects and produces metadata for collections and items in those collections. Metadata Object Description Schema (MODS) is the standard schema used, but other schemata ``may be employed to support different projects/collections'' [@DigitalCollections, Organization and metadata].\nBefore collections are digitized by Williams College Special Collections, they must be organized and described, but ``item-level metadata may be created as part of the development of a digital collection'' [@DigitalCollections, Organization and metadata].\nDigital Preservation Services # Williams College Special Collections is the designated repository for rare books, records of Williams College, select professional work of faculty and alumni, and other collections in support of the curriculum. As such, Special Collections ``assumes primary responsibility for long term preservation of its digital holdings'' [@DigitalCollections, Preservation].\nMaintenance and Removal # Special Collections aims to develop ``digital collections that are of high quality, useful and usable, and cohesive'' [@DigitalCollections, Maintenance and removal]. Doing so may necessitate removal of individual objects or entire collections ``for reasons of violation of copyright or copyright dispute, inaccurate data or facts, collection weeding, storage, or the material is no longer in support of the Williams scholarly community'' [@DigitalCollections, Maintenance and removal]. In some cases, removal will be noted in metadata in order to preserve historical record [@DigitalCollections, Maintenance and removal].\n``Collections as Data'' # Browsing the Williams College Library and Special Collections web pages, I was not able to find any mention or description of text data mining services or AI/ML services. When used with the website's search tool, the search terms ``text data mining'' and ``AI/ML'' did not return any results on the College's library or special collections web pages. Instead, results pointed to course offerings within the College's computer science department. Nevertheless, Special Collections does provide a web archiving service, in effect building a collection that could be mined as data, but does not appear to have been mined yet, at least under the auspices of Williams College Special Collections [@WebArchiving].\n"},{"id":15,"href":"/portfolio/docs/outcomes/evaluate/mqp/","title":"LS 566: Metadata Quality Problems","section":"Evaluating Technology-Mediated Access in Library and Information Services","content":"William Blackerby\nDr. Steven MacCall\nLS 566\n25 September 2021\nAssignment #2: Metadata quality problems\nIn their article \u0026ldquo;Achieving and Maintaining Metadata Quality: Toward a Sustainable Workflow for the IDEALS Institutional Repository,\u0026rdquo; Stein, Applegate, and Robbins give a useful overview of the kinds of metadata quality problems that can occur in collections before detailing how they developed a workflow to address metadata problems in the institutional repository they manage. Here I will summarize the kinds of problems they discuss.\nThe authors begin by noting that there is no universally accepted definition of quality metadata before sharing Park\u0026rsquo;s proposed definition (645). Following Park, we can determine if we have quality metadata by asking if it performs \u0026ldquo;the core bibliographic functions of discovery, use, provenance, currency, authenticity, and administration,\u0026rdquo; or more generally, asking if the metadata is complete, accurate, and consistent (645). Yasser\u0026rsquo;s \u0026ldquo;five universal metadata problems\u0026rdquo; of \u0026ldquo;Incorrect Values, Incorrect Elements, Missing Information, Information Loss, and Inconsistent Value Representation\u0026rdquo; are perhaps even more useful for understanding metadata quality problems thanks to their concreteness (645-646).\nMultiple sources and entry points for metadata into a system can lead to incompleteness, inaccuracy, and inconsistency. Both humans and machines can introduce these kinds of problems, and problems multiply when metadata in legacy formats like MARC are reused and when there is exchange between systems (647). Metadata from sources that use different controlled vocabularies can lead to inconsistencies, as can the lack of a name authority file (647). There may be syntactic inconsistencies in metadata (e.g., differences in capitalization, abbreviation) as well semantic differences like the conflation of contributors and authors (659).\nIn addition to these technical issues, administrative challenges can also contribute to metadata quality problems. Staffing and funding are required for the creation and maintenance of quality metadata, but budget restraints can impede having adequate numbers of workers with the right tools and training to assure metadata quality (647). The authors also note that IT staff and librarians who collaborate on these types of projects belong to \u0026ldquo;different networks of practice\u0026rdquo; and thus may have challenges with communicating about their preferred tools and methods (650).\nThis article accomplishes one of its stated goals of addressing \u0026ldquo;theoretical issues regarding metadata quality\u0026rdquo; and serves as a thorough introduction to the kinds of problems metadata workers might encounter (645). Their details of the development of the workflow for their own institutional repository are equally interesting in proposing solutions to metadata quality problems, but discussion of that element of the article will have to wait for another day.\nWorks Cited\nAyla Stein, Kelly J. Applegate \u0026amp; Seth Robbins (2017) Achieving and Maintaining Metadata Quality: Toward a Sustainable Workflow for the IDEALS Institutional Repository, Cataloging \u0026amp; Classification Quarterly, 55:7-8, 644-666, DOI: 10.1080/01639374.2017.1358786\n"},{"id":16,"href":"/portfolio/docs/technology/metadata/","title":"Metadata Workflow Series","section":"Technology","content":" Hello # "},{"id":17,"href":"/portfolio/docs/philosophy/","title":"Philosophy of LIS Practice","section":"Docs","content":" A Reflective Essay on Information, Technology, Humanity, and Purpose # I distinctly remember what first piqued my interest in computing and technology. I was in sixth or seventh grade, spending the night at a friend\u0026rsquo;s house, and before settling in for a late night and early morning of action movie after action movie followed by far too little sleep, we sat at the desktop computer in the basement guestroom. Instead of playing through one of his favorite science fiction flight simulator games, my friend showed me how some text surrounded by angle brackets he typed out in NotePad could turn into a website anyone anywhere in the world could visit thanks to AngelFire. HTML: HyperText Markup Language. Language. I knew language. I loved language! French and English were my two favorite classes in school at the time. All it took to get a computer to do what you wanted was to learn its language? I could do that. I was hooked.\nIn the almost 25 years since that night, that interest in the intersection of language and technology has gone through ups and downs, wavering but never flickering out. I shudder to think how much money I\u0026rsquo;ve spent on programming language books and courses that I never finished \u0026ndash; Java, PHP, Ruby, Python, Perl, C, Haskell, OCaml, Standard ML, Scheme, Racket, Common Lisp, JavaScript, Clojure, Elm, Go, Smalltalk, Prolog, R, and others I\u0026rsquo;m sure I\u0026rsquo;m forgetting. Once I realized how much math figured into this field, something I unfortunately decided I wasn\u0026rsquo;t good at a young age because I wasn\u0026rsquo;t as fast at it as my peers (and in which I tried to justify or feign disinterest as I announced that it wasn\u0026rsquo;t necessary for what I wanted to do), I more or less gave up, again and again, immersing myself in and identifying with the more human side of my interest in language, ultimately earning a bachelor\u0026rsquo;s degree in classical languages with an emphasis on Ancient Greek.\nI think what ties these scattershot interests together is an interest in structure, form, and, most importantly, meaning. What motivates me now, I think, is learning how to use these interests to help people, which has been the focus of my career up to this point. Each job I have held since college, from working as a community organizer in inner-city Birmingham, serving as a youth minister at an Episcopal church in Huntsville, and now teaching Latin at a private school in suburban Birmingham, has involved navigating, organizing, and interpreting information. In each position, there have been situations in which I have asked or been asked to answer questions that should be easy to answer, but haven\u0026rsquo;t been because of a lack of underlying form, structure, or organization to the information available to help answer the question. In other words, a lack of meaning, or semantic context. So my goal is this: to develop interpersonal and technical skills that will help human-serving organizations confidently manage the information at their disposal.\nMy understanding of information is becoming grounded in Buckland\u0026rsquo;s conceptualization of information as thing, because as Buckland says, \u0026ldquo;expert systems and information retrieval systems\u0026rdquo; \u0026ndash; in other words, technological systems for working with information \u0026ndash; \u0026ldquo;can only deal with information in the sense of information as thing\u0026rdquo; (352). Human-serving organizations abound with instances of information as thing: policy documents, meeting minutes, emails, blog posts, newsletters, bills, receipts, spreadsheets, invoices, and more. I believe that technology has a role to play in managing these informative objects but that in many cases, the necessary skills are lacking from these organizations. I want to be someone who can bring such skills to the table for these human-serving organizations, all while being careful to avoid implementing technology for technology\u0026rsquo;s sake and instead recalling Ma\u0026rsquo;s dictum that\nThe design of information systems and the education of information professionals depend upon a sound theory of communication, one that describes and explains how humans really interact with each other and how meanings are constructed through acts of understanding. (718-719)\nIn other words: the answer to the question \u0026ldquo;What is the purpose of all of this?\u0026rdquo; is understanding and collaborative meaning construction.\nAs an aspiring information professional and technologist, I hope to carry the message of Zobel\u0026rsquo;s work on information retrieval (IR) into my practice. What so impressed me about the paper of his that we read this semester is that purpose is at the center of his conceptualization of IR. I found the following definition of IR from his paper inspiring: \u0026ldquo;the study of techniques for supporting human cognition with documents, using material that is sourced from large document collections\u0026rdquo; (25).\n\u0026ldquo;Supporting human cognition.\u0026rdquo; Not replacing, but supporting. In other words, being mindful of who and what this work is for. Does artificial intelligence have a role to play in any of this? If so, what do we mean by artificial intelligence, and what is that role?\nOne place to start as we attempt to answer these questions is with the work of Alan Turing, the mathematician and computer scientist whom we have to thank (or perhaps from some perspectives, to blame) for computing as we know it today. This essay is not an appropriate venue for a thorough discussion of Turing\u0026rsquo;s work, nor am I anywhere near expert enough in it to make any claims except for this: that we cannot separate the history of computing from the history of artificial intelligence. For more on this idea, Sebastian Sunday Grève\u0026rsquo;s recent essay for Aeon is a good place to start.\nWe should also acknowledge that there is an undeniably darker, and in some cases sinister, side to artificial intelligence as well. Introducing a series of articles on \u0026ldquo;AI colonialism\u0026rdquo; for the MIT Technology Review, Karen Hao writes \u0026ldquo;The more users a company can acquire for its products, the more subjects it can have for its algorithms, and the more resources—data—it can harvest from their activities, their movements, and even their bodies.\u0026rdquo; To what end? Writing for Politico\u0026rsquo;s Digital Future Daily newsletter, Derek Robertson synthesizes a Twitter thread in which Emily Bender, a computational linguist at the University of Washington, criticizes Eric Jang\u0026rsquo;s essay in which he lays out his \u0026ldquo;ambitions to build an artificial general intelligence that would encapsulate the entirety of human experience.\u0026rdquo; Robertson says the thread criticizing the essay is\na useful case study in two different technological visions of the world: One where progress is inevitable, competition is zero-sum, and engineers should be single-mindedly focused on getting there first, and one where technological advancement is more holistic and human-centered.\nJust how far does the former of these two visions go? In an essay for Current Affairs, Phil Torres investigates a philosophy called longtermism, \u0026ldquo;the idea that what matters most is for “Earth-originating intelligent life” to fulfill its potential in the cosmos.\u0026rdquo; Though it may seem initially unrelated to AI, Torres says that \u0026ldquo;many longtermists believe that superintelligent machines pose the greatest single hazard to human survival, but they seem convinced that if humanity were to create a “friendly” superintelligence whose goals are properly “aligned” with our “human goals,” then a new Utopian age of unprecedented security and flourishing would suddenly commence.\u0026rdquo;\nThe latter, and much less extreme vision, is illustrated on the about page of Timnit Gebru\u0026rsquo;s DAIR, the Distributed AI Research Institute, \u0026ldquo;an interdisciplinary and globally distributed AI research institute rooted in the belief that AI is not inevitable, its harms are preventable, and when its production and deployment include diverse perspectives and deliberate processes it can be beneficial.\u0026rdquo;\nThis is the vision we should strive for, the vision that is in line with Ma\u0026rsquo;s and Zobel\u0026rsquo;s views, and the vision that I would want to be a part of. Information is for people, all people, and technology that does not recognize and advance that point of view can be harmful and dangerous. I am grateful to be taking my first steps into LIS, a field which, though technologically informed, places people\u0026rsquo;s needs first.\nReferences:\nBuckland, M. K. (1991). Information as thing. Journal of the American Society for Information Science, 42 (5), 351-360.\nDAIR. \u0026ldquo;About.\u0026rdquo; https://www.dair-institute.org/about. Accessed May 1, 2022.\nGrève, Sebastian S. \u0026ldquo;AI’s first philosopher.\u0026rdquo; https://aeon.co/essays/why-we-should-remember-alan-turing-as-a-philosopher. Accessed May 1, 2022.\nHao, Karen. \u0026ldquo;Artificial intelligence is creating a new colonial world order.\u0026rdquo; https://www.technologyreview.com/2022/04/19/1049592/artificial-intelligence-colonialism/. Accessed May 1, 2022.\nMa, L. (2012). Meanings of information: The assumptions and research consequences of three foundational LIS theories. Journal of the American Society for Information Science, 63 (4), 716-723.\nRobertson, Derek. https://www.politico.com/newsletters/digital-future-daily/2022/04/29/cryptos-strange-new-respectability-00029062. Accessed May 1, 2022.\nZobel, J., (2017). What we talk about when we talk about information retrieval. ACM SIGIR Forum, 51 (3), 18-26.\n"},{"id":18,"href":"/portfolio/docs/outcomes/practice/","title":"Practicing Principles of Social and Cultural Justice","section":"Program Learning Outcomes and Work Samples","content":" Practicing principles of social and cultural justice in my preparation for a career in library and information environments # "},{"id":19,"href":"/portfolio/docs/outcomes/","title":"Program Learning Outcomes and Work Samples","section":"Docs","content":" Program Learning Outcomes and Work Samples # Students will: # evaluate technology-mediated access in library and information services # use evidence to inform library and information practices # critically articulate the philosophy, principles, and ethics of library and information science # practice principles of social and cultural justice in their preparation for careers in library and information environments # "},{"id":20,"href":"/portfolio/docs/resume/","title":"Resume","section":"Docs","content":" William T. Blackerby # 332 Willow Bend Road\nBirmingham, AL 35209\n205-240-3705\nwmblackerby@gmail.com • LinkedIn • Github • Blog\nEducation # The University of Alabama, Tuscaloosa, AL\nMaster of Library and Information Studies (online)\nexpected completion Spring 2023\nCompleted coursework: LS 500: Information Science and Technology, LS 501: Information and Communities, LS 535: Records Management, LS 566: Metadata and Semantic Web Fundamentals, LS 590: Linked Data, LS 506: Modern Cataloging and Classification, LS 562: Digital Libraries, LS 564: Programming for Digital Libraries, LS 570: Internship (Law Library of Congress Remote Metadata Internship) Coursework in progress Spring 2023: LS 513: Professional Paths, LS 569: Information Management, LS 570: Internship (Southern Music Research Center) The University of the South, Sewanee, TN\nBachelor of Arts, cum laude, 2009\nMajor: Classical Languages Omicron Delta Kappa (national leadership honor society) Order of the Gown (academic honor society), 2007-2009 Experience # Law Library of Congress Remote Metadata Intern, August 2022-December 2022\nPart of a team assigned to extract text and metadata from Congressional Research Service bill digests for the 74th-76th Congresses Developed several Python and shell scripts to automate aspects of the workflow Indian Springs School\nLatin and Greek Teacher, August 2016-Present\nTeach Latin I, II, III, IV, AP Latin and Introduction to Ancient Greek Modern and Classical Languages Department Chair, July 2021-Present\nSupervise five World Languages teachers Manage departmental budget Dean of Students, July 2019-June 2021\nMember of Administrative Leadership Team Managed Student Activities budget Managed student records including vehicle registrations, permission forms, discipline letters Oversaw student government activities, including elections twice per year Greater Birmingham Ministries, Birmingham, AL\nFaith in Community Organizer, July 2009 – January 2013\nHired, trained, and supervised six part-time staff for 2012 non-partisan voter engagement campaign Maintained organization’s website, blog, and Facebook and Twitter accounts Coordinated production of quarterly newsletter, including writing and editing articles Represented organization at public events, including congregational mission/outreach fairs Technologies # Programming languages: Ruby (intermediate), Python (intermediate), R (intermediate), PHP (intermediate) Web technologies: HTML (intermediate), Markdown (intermediate) Data technologies: Google Sheets (intermediate), Microsoft Excel (intermediate), SPARQL (intermediate), OpenRefine (intermediate), MySQL (intermediate), SQLite (intermediate) Tools: Unix command line (intermediate), Git version control system (intermediate), bash/zsh shell scripting (intermediate), awk (intermediate), sed (intermediate), miller (intermediate), emacs (intermediate), vim (beginning) Languages # Latin (Full professional proficiency) French (Limited working profiency) Ancient Greek (Limited working proficiency) Community Involvement # Alabama Arise/Alabama Arise Action\nBoard of Directors, 2014-2020\nStatewide organization advocating for state policies to improve the lives of low-income Alabamians Served one term as President of the Board of Directors Activities included: chairing board meetings and conference calls; emceeing special events; consulting with executive director on finance, development, programs, hiring, and special projects, e.g., organizational commitment to racial equity and inclusion; committee service including finance, development, nominations, and personnel; conducting executive director performance review. Gasp, Inc.\nBoard of Directors, 2011-2021\nMembership-based organization advancing healthy air and environmental justice in the greater Birmingham area through education, advocacy, and collaboration Multiple terms as Secretary of the Board of Directors Activities included: taking minutes at every board meeting, serving on development committee, participating in executive director performance review. Awards and Honors # Eagle Scout, 2005 Second Place, Koine Greek, Eta Sigma Phi Maurine Dallas Watkins Sight Translation Contest, 2007 Isaac Marion Dwight Medal for Philosophical Greek, University of the South, 2009 "},{"id":21,"href":"/portfolio/docs/outcomes/use/","title":"Using Evidence to Inform Library and Information Practices","section":"Program Learning Outcomes and Work Samples","content":" Using evidence to inform library and information practices # LS 535 Memo\n"},{"id":22,"href":"/portfolio/docs/technology/2022-12-02-senate-topic-modeling/","title":"Visualizing Topics of 74th Congress Senate Bills and Joint Resolutions","section":"Technology","content":" Visualizing Topics of 74th Congress Senate Bills and Joint Resolutions # This code is available to experiment with on Binder in a notebook with the filename tmr.ipynb. Be sure to check out the ner_mapping.ipynb notebook as well, in which I create a heatmap of states mentioned in Senate bills and joint resolutions during the 74th Congress.\nIn LS 562: Digital Libraries, we studied the application of machine learning tools to text analysis problems. For practice, I decided to apply one of those tools, topic modeling, to the corpus of 74th Congress Senate bills and join resolutions I processed during my remote metadata internship with the Law Library of Congress.\nSetting Up # The Necessary Libraries # Below, we load the libraries this project uses into my work environment. tidyverse, tidytext, and ggplot2 are pretty well know inhabitants of the R Tidyverse, so I won\u0026rsquo;t explain them here. stm is the package used in this project for topic modeling. LDAvis is the tool we will use to visualize the LDA model applied to Senate bill corpus. quanteda is a text analysis library, and spacyr is an R wrapper for the Python natural language processing library spaCy. gistr is necessary for publishing the visualization to the web.\nlibrary(tidyverse) library(tidytext) library(ggplot2) library(stm) library(LDAvis) library(quanteda) library(spacyr) library(gistr) Initializing spaCy # Below, we initialize spaCy with its built-in English language model.\nspacy_initialize(model = \u0026#34;en_core_web_sm\u0026#34;) Load dataset # The data we will use is available for download as a CSV file from a simple API created using Datasette. I have crafted a SQL query to get just the data that we need. This query will return a table containing each bill summary\u0026rsquo;s unique ID in the database and its text.\ndata \u0026lt;- read.csv(\u0026#34;https://llc.herokuapp.com/summaries.csv?sql=select%0D%0A++rowid%2C%0D%0A++content_text%0D%0Afrom%0D%0A++files%0D%0Awhere%0D%0A++%22path%22+like+%2274_2_s%25%22%0D%0Aorder+by%0D%0A++path\u0026amp;_size=max\u0026#34;) head(data) Preprocessing # With the environment set up and the data acquired, we can start massaging the data into the shape the topic modeling algorithm needs.\nspaCy # First, we put the text of all the bills we downloaded into one big string which we then use spaCy to parse.\ntext \u0026lt;- data$content_text %\u0026gt;% str_c() parsed_text \u0026lt;- spacy_parse(text, lemma = TRUE, entity = TRUE, nounphrase = TRUE) head(parsed_text) Getting the words we want # Next, we reduce the corpus to tokens that have been part-of-speech tagged as proper nouns, verbs, nouns, adjectives, and adverbs. The part-of-speech tagging isn\u0026rsquo;t perfect, so we remove tokens tagged as those parts of speech that we know aren\u0026rsquo;t, actually. Finally, make all of the lemmas lowercase and create the word column.\ntokens \u0026lt;- parsed_text %\u0026gt;% filter(pos == \u0026#34;PROPN\u0026#34; | pos == \u0026#34;VERB\u0026#34; | pos == \u0026#34;NOUN\u0026#34; | pos == \u0026#34;ADJ\u0026#34; | pos == \u0026#34;ADV\u0026#34;) %\u0026gt;% filter(lemma != \u0026#34;.\u0026#34; \u0026amp; lemma != \u0026#34;Mr.\u0026#34; \u0026amp; str_detect(lemma, \u0026#34;^\\\\w\\\\.?$\u0026#34;, negate = TRUE)) %\u0026gt;% filter(lemma != \u0026#34;§\u0026#34;) %\u0026gt;% mutate(word = tolower(lemma)) Next we get rid of stop words\ndata(\u0026#34;stop_words\u0026#34;) tokens \u0026lt;- tokens %\u0026gt;% anti_join(stop_words) And now we can take a quick look at the tokens that appear more than 100 times in the bill summaries.\ntokens %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 100) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) The names of months show up a lot, but they won\u0026rsquo;t contribute meaningfully to the topic model. Let\u0026rsquo;s get rid of them\u0026hellip;\nmonth_tokens \u0026lt;- tibble(month.name) %\u0026gt;% unnest_tokens(word, month.name) tokens \u0026lt;- tokens %\u0026gt;% anti_join(month_tokens) \u0026hellip;and see what our chart looks like now\ntokens %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 100) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) This is better, but there\u0026rsquo;s more we can do. First, let\u0026rsquo;s remove some more words that might clutter our model.\nThese calls bring the text of bill types and the names of sponsors identified in the original document into the environment.\nbill_types \u0026lt;- read.csv(\u0026#34;https://llc.herokuapp.com/summaries.csv?sql=select+distinct+bill_type+from+actions\u0026amp;_size=max\u0026#34;) sponsors \u0026lt;- read.csv(\u0026#34;https://llc.herokuapp.com/summaries.csv?sql=select+distinct+sponsor+from+actions+where+sponsor+is+not+null+and+sponsor+%21%3D+%22%22\u0026amp;_size=max\u0026#34;) We can now make that text tokens\nbill_type_tokens \u0026lt;- bill_types %\u0026gt;% unnest_tokens(word, bill_type) sponsor_tokens \u0026lt;- sponsors %\u0026gt;% unnest_tokens(word, sponsor) and remove them from our corpus.\ntokens \u0026lt;- tokens %\u0026gt;% anti_join(bill_type_tokens) %\u0026gt;% anti_join(sponsor_tokens) How does our chart look now?\ntokens %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 100) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) It doesn\u0026rsquo;t look like any change, but we have reduced the number of tokens in our corpus. I think we\u0026rsquo;re in good enough shape to do our topic modeling.\nTopic Modeling # A document feature matrix # The stm() function exposed by the stm package can\u0026rsquo;t take our data frame as input. Instead, it needs a document-term matrix, in this case in the form of a document-feature matrix, which we create with the tidytext cast_dfm() function.\ndfm \u0026lt;- tokens %\u0026gt;% count(doc_id, word, sort = TRUE) %\u0026gt;% cast_dfm(doc_id, word, n) We\u0026rsquo;re ready to apply the model! I\u0026rsquo;ve set k, the number of topics, as 48 because there were 48 Senate committees in 74th Congress. The following line of code takes a few minutes to finish executing.\nmodel \u0026lt;- stm(dfm, K = 48, init.type = \u0026#34;LDA\u0026#34;, seed = 1234, verbose = FALSE) Visualization # Before we can visualize our model with LDAvis, we need to put our document-term matrix into a form that toLDAvis, an stm-provided wrapper around LDAvis, can understand. We do that with the call to convert() below. The call to toLDAvis creates the visualization as a static website and publishes it online.\nlda \u0026lt;- convert(x = dfm, to = \u0026#34;lda\u0026#34;) ldavis \u0026lt;- toLDAvis(model, lda$documents, R = 48, open.browser = interactive(), as.gist = TRUE) You can see the final product here.\nWrapping up # Poking around in the visualization a little bit, it becomes clear that there\u0026rsquo;s some more refinement of our data we could do, e.g., removing any token with a numeral in it, removing more honorifics than just \u0026ldquo;Mr.\u0026rdquo;, and perhaps removing the names of actions like \u0026ldquo;approve\u0026rdquo;, \u0026ldquo;pass\u0026rdquo;, and \u0026ldquo;refer\u0026rdquo;. That said, I think that this visualization does offer a good sense of the topics under discussion in various Senate committees in the 74th, and creating it was a great skill-building exercise.\n"}]