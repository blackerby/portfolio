[{"id":0,"href":"/portfolio/docs/technology/metadata/2022-11-22-llc-workflow/","title":"1. A High Level Metadata Workflow Overview","section":"Metadata Workflow Series","content":" A High Level Metadata Workflow Overview # The semester is winding down and so is my time as a remote metadata intern for the Law Library of Congress. I ended up learning a ton about Unix text processing workhorses like aspell, awk, diff, find, grep, sed, and GNU Emacs as well as newer tools like Miller and Datasette. I incorporated these tools into several shell scripts to expedite my workflow, which is built around a python script I wrote to extract specific text from the PDF I was working with using regular expressions. I also did a fair amount of work with Google Sheets and SQLite, and towards the end of the internship I even dabbled with the Google Drive API.\nMy plan now is to write a series of posts detailing the steps in this workflow and my use of the tools mentioned above. Below is a high-level overview of my workflow as it stands towards the end of the semester.\nExtract text with the prep.sh script. Paste initial metadata into Google Sheets. This initial metadata comes from running the command mlr --c2t --headerless-csv-output cat 74_2_{PAGE_NUM}.csv | pbcopy. 74_2_{PAGE_NUM}.csv is one of the files generated by the the prep.sh script. Review each of the text files generated by prep.sh (this includes running aspell). Run cleanup.sh to delete backup files, move the working directory to its final location, and copy the names of the text files to the clipboard for pasting into Google Sheets. Run update_tsv_data.sh to create local copies of the metadata, load the metadata and text files into a SQLite database, and publish that database to Heroku. Stay tuned for more posts about each of the above steps, starting with the prep.sh script.\n"},{"id":1,"href":"/portfolio/docs/technology/metadata/2022-11-23-prep/","title":"2. prep.sh and mlr","section":"Metadata Workflow Series","content":" prep.sh and mlr # I am an inexperienced shell scripter, but this project provided opportunities to do some productive work with the shell. My workflow starts with running prep.sh, a script that takes 2 or 3 arguments and is explained below.\nShebang # I\u0026rsquo;m on macOS, so my shebang line references zsh.\n#! /usr/bin/env zsh Arguments # FILE=$(realpath $1) # full path to file name. must be pdf START=$2 # starting page of pdf file if [[ -n $3 ]] then FINISH=$3 # ending page of pdf file fi Here, I use realpath to take an argument like ../../74.2.pdf and expand it to its full path. The first argument, $1, is should be a path to a PDF file. $2 should be the page the python script begins processing from, and $3 should be the last page it processes.\nProcessing # # pull metadata into csv and write summaries out to text files # named following this schema: {congress}_{session}_{bill type}_{bill number}.txt if [[ -n $FINISH ]] then python3 get_summaries.py $FILE $START $FINISH else python3 get_summaries.py $FILE $START fi If there is no $3, no worries: we just pass the file path and the start page to the python script.\n# get file\u0026#39;s name for name of new directory and iteration purposes FILE_BASENAME=$(basename $FILE .pdf) Here, I use basename with the name of the file and the extension .pdf to get the basename of the file the script is processing. This will be used to name the directories created by the script.\n# make new directory and move generated files into it if [[ -n $FINISH ]] then NEW_DIR=\u0026#34;${FILE_BASENAME}_${START}_${FINISH}\u0026#34; else NEW_DIR=\u0026#34;${FILE_BASENAME}_${START}\u0026#34; fi Above, we name the directory that the files generated by the script will be moved into.\nManual processing of the bill summaries pointed out some common OCR errors, so below I run a sed script to deal with some of those.\nfor f in *.{txt,csv} do gsed -f common_errors.sed -i $f done Here\u0026rsquo;s what that script looks like:\ns/\\bJudiciarv\\b/Judiciary/ig s/\\bTvdings\\b/Tydings/g s/\\bbv\\b/by/g s/\\banv\\b/any/g s/\\bdenved\\b/derived/g s/\\bm\\b/in/g s/\\bpiopose\\b/propose/g s/\\bwhioh\\b/which/g s/\\beto.\\b/etc./g s/\\bwit inn\\b/within/g s/\\bwrorks\\b/works/g s/\\bLnited\\b/United/g s/\\bsolelv\\b/solely/g s/\\blavs\\b/lays/g s/\\bArmv\\b/Army/g s/\\biniured\\b/injured/g s/\\bDedares\\b/Declares/ig s/\\bcrcp\\b/crop/ig s/\\bWilev\\b/Wiley/ig s/\\bdomestio\\b/domestic/ig Simple stuff, but it saves some time.\nThe OCR preserved the typesetting of the lines in the original document, so I run another sed script to make the lines flow and to replace some of the entities the script brings in and normalize some of the punctuation.\nfor f in *.txt do gsed -i.bak -Ez -f clean_lines.sed $f done Here\u0026rsquo;s that script:\ns/- \\n//g s/(\\.:) \\n/\\1\\n\\n/g s/([^.:] )\\n/\\1/g s/(‘‘|’’|“|”)/\u0026#34;/g s/[‘’]/\u0026#39;/g s/-?— ?/--/g s/■//g s/ (#|\\.+) / /g s/(\\w\\.)\\.+/\\1./g Wrapping up # Finally, I move the files generated by the python script into a directory named after the file and page number(s) the being procssed and then copy that directory and its contents into a backup directory.\nmkdir $NEW_DIR mv $FILE_BASENAME*.??? $NEW_DIR cp -R $NEW_DIR \u0026#34;${NEW_DIR}.bak\u0026#34; Pasting metadata # The data entry for this workflow happens in Google Sheets, so I use Miller to convert the generated CSV file to TSV for easier pasting into Sheets. I could probably go back and have the script spit out TSV instead of CSV.\nmlr --c2t --headerless-csv-output cat 74_2_{PAGE_NUM}.csv | pbcopy --c2t tells Miller to convert CSV to TSV, and --headerless-csv-output ignores the first line of the file.\nUp next # As you can probably tell, this script is really just a harness for the get_summaries.py script that does the bulk of the work. That script will be explained in the next post.\n"},{"id":2,"href":"/portfolio/docs/technology/metadata/2022-11-24-get-summaries/","title":"3. get_summaries.py","section":"Metadata Workflow Series","content":" get_summaries.py # I hope to convert this file to ipynb format so interested folks can try the code out on their own. The code is currently published on GitHub.\nEnvironment Setup # The Shebang Line # We add a shebang line in case we want to make this file executable.\n#!/usr/bin/env python Necessary Libraries # Now we import the libraries we need.\nimport argparse import sys import re import csv from pathlib import Path from more_itertools import chunked from PyPDF2 import PdfReader The role of each library is explained below:\nargparse: handles command-line argument parsing In this script, those command line arguments are the path to the PDF file we are processing, the page we will start processing from, and an optional argument for where we will stop processing. sys: handles a variety of interactions between python and its host system In this script, it is used once: to exit the program in case of a fatal error re: is python\u0026rsquo;s regular expression library In this script, it does the bulk of the work. csv: is python\u0026rsquo;s library for dealing delimited data files. In this script, it is used to write metadata from the bill summaries to a csv file. from pathlib import Path: imports the Path object for dealing with file paths from more_itertools import chunked: imports the chunked function from the more_itertools library more_itertools is not part of a standard python distribution and needs to be installed using pip. from PyPDF2 import PdfReader: is a third-party library for dealing PDF files that be installed using pip. Constants # (No) Magic Numbers # There are three numbers that play key roles in the script. To avoid having magic numbers, we make them constants with descriptive names.\nELEMENT_COUNT = ( 7 # full header, bill type, bill number, sponsor(s), action date, committee, text ) CONGRESS_POSITION = 2 SESSION_POSITION = 3 ELEMENT_COUNT: is adequately explained by the comment in the above code block. CONGRESS_POSITION: refers to starting index of the number in the name of the file passed to the script that refers to the congress (e.g., 74th) the summaries were written for. SESSION_POSITION: is like CONGRESS_POSITION, but for the session of Congress (e.g., second). The Month Dictionary # We use a python dictionary to map between names (or abbreviations, or bad OCR text) of months as found in the bill summaries and two character strings of digits that will be used in naming the files containing the extracted summaries.\nMONTHS = { \u0026#34;January\u0026#34;: \u0026#34;01\u0026#34;, \u0026#34;February\u0026#34;: \u0026#34;02\u0026#34;, \u0026#34;Feb.\u0026#34;: \u0026#34;02\u0026#34;, \u0026#34;March\u0026#34;: \u0026#34;03\u0026#34;, \u0026#34;Mar.\u0026#34;: \u0026#34;03\u0026#34;, \u0026#34;April\u0026#34;: \u0026#34;04\u0026#34;, \u0026#34;Apr.\u0026#34;: \u0026#34;04\u0026#34;, \u0026#34;May\u0026#34;: \u0026#34;05\u0026#34;, \u0026#34;June\u0026#34;: \u0026#34;06\u0026#34;, \u0026#34;July\u0026#34;: \u0026#34;07\u0026#34;, \u0026#34;August\u0026#34;: \u0026#34;08\u0026#34;, \u0026#34;September\u0026#34;: \u0026#34;09\u0026#34;, \u0026#34;October\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;November\u0026#34;: \u0026#34;11\u0026#34;, \u0026#34;December\u0026#34;: \u0026#34;12\u0026#34;, \u0026#34;Alay\u0026#34;: \u0026#34;05\u0026#34;, # bad OCR, should be \u0026#34;May\u0026#34; } Big, Ugly Regular Expressions # The script relies on two regular expressions to do its work. They\u0026rsquo;re big, they\u0026rsquo;re ugly, they\u0026rsquo;re poorly tested, and they match a lot more than they are intended to match, but this script would be useless without them. In a later draft of this post, and a later version of the script, I will rewrite the regular expressions with the verbose flag, which they obviously need. I decided to demonstrate the version of the regexes the script actually uses.\nThe Summary Header Pattern # This regular expression (usually) matches the first line of a bill summary and groups (see the second row of the table) parts of the line that will be extracted into a starter CSV metadata file. The abundance of punctuation options has to do primarily with inconsistency in how OCR recognizes the punctuation characters in the original text, but there is also some inconsistency in how the original text is punctuated.\nHEADER_PATTERN = re.compile( r\u0026#34;(((?:S|H)\\.? ?(?:R\\.?)? (?:J\\.? Res\\. ?)?)(\\w{1,5})\\.? ((?:M(?:rs?|essrs)\\.) .+?)(?:[;,:])? (\\w{1,9} \\d{1,2}[.,] \\d{4})[.—]? ?\\n?(?:\\(([\u0026#39;0-9a-zA-Z ]+)\\))?(?:\\.|.+\\.|:|.+:)?)\u0026#34;, re.MULTILINE, ) The regex captures the bill type, bill number, sponsors, date of introduction, committee, and text of the bill summary. It also captures the entire first line of the summary.\nThe Date Pattern # This regular expression is intended to match dates like the following: January 16, 1936. The year is optional.\nDATE_PATTERN = re.compile(r\u0026#34;([JFMASOND][a-z]{2,8}\\.?) (\\d{1,2})[-—.,;: ]( \\d{4})?\u0026#34;) Functions # Now we turn our attention to the functions that will be called in the main body of the script.\nArgument Parsing # Here, we use the argparse package to handle three command line arguments: the name of the file, the page to start from, and the page to end on.\ndef parse_args(): parser = argparse.ArgumentParser() parser.add_argument( \u0026#34;file_path\u0026#34;, help=\u0026#34;path to a bill digest PDF, e.g., ../74_2.pdf\u0026#34; ) parser.add_argument( \u0026#34;start_page\u0026#34;, help=\u0026#34;PDF page number (not printed page number) for where program should start working\u0026#34;, type=int, ) parser.add_argument( \u0026#34;end_page\u0026#34;, help=\u0026#34;last PDF page number (not printed page number) program should process\u0026#34;, type=int, nargs=\u0026#34;?\u0026#34;, ) return parser.parse_args() Naming the CSV data file # The CSV file we generate needs a meaningful name, e.g., 74_2_7_8.csv, that is, the Congress and Session (74_2, which we get from the file stem of the PDF file we\u0026rsquo;re processing, followed by the start page and the end page (if one is specified).\ndef name_output_file(file_stem, start_page, end_page): if end_page: return \u0026#34;_\u0026#34;.join([file_stem, str(start_page), str(end_page)]) + \u0026#34;.csv\u0026#34; else: return \u0026#34;_\u0026#34;.join([file_stem, str(start_page)]) + \u0026#34;.csv\u0026#34; Getting the Data # This is where most of the work happens. The function could and should probably be broken out into some smaller functions, but it works. See the comments in the function definition for details, and the moreitertools chunked documentation for information about the use of that function.\ndef extract_summaries_and_metadata(file_path, start_page, end_page): # Set up some empty containers for our data metadata = [] summaries = [] text_file_names = [] # Instantiate the PdfReader object reader = PdfReader(file_path) # Get the file stem, e.g., 74_2 file_stem = file_path.stem # Handle the presence of absence of an ending page number if end_page == None: end = start_page + 1 else: end = end_page # Iterate over page numbers for i in range(start_page, end): # Get the text from each page page_text = reader.pages[i].extract_text() # Find the first line that matches HEADER_PATTERN from above first_header_pos = re.search(HEADER_PATTERN, page_text).start() # Get the text from that point on page_text = page_text[first_header_pos:] # Split the text based on the header pattern raw_summaries = re.split(HEADER_PATTERN, page_text)[1:] # Group the summaries raw_summaries = list(chunked(raw_summaries, ELEMENT_COUNT)) for item in raw_summaries: # Give a name to each grouped piece of each summary header, bill_type, bill_number, sponsor, date, committee, text = item # Normalize the bill type formatting formatted_bill_type = bill_type.strip() # Normalize bill type for the summary text file name lower_bill_type = bill_type.lower().replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;).replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) # Concatenate the summary header and summary text summary = header + text # Add a row of metadata to the list metadata.append( [formatted_bill_type, bill_number, sponsor, date, committee] ) # Add a summary to the list summaries.append(summary) # Add the name of the text file to the list text_file_names.append(f\u0026#34;{file_stem}_{lower_bill_type}{bill_number}\u0026#34;) # return a tuple of lists of data return (metadata, summaries, text_file_names) Dates # Here\u0026rsquo;s an example of a bill summary in the original PDF:\nAnd here\u0026rsquo;s what it looks like after processing:\nS. 1019. Mr. King; January 15, 1935 (District of Columbia).\nAs passed by the Senate, February 12, and referred to House Committee on the District of Columbia. February 15, 1935:\nRequires real estate brokers and salesmen in the District of Columbia (including nonresidents but excluding auctioneers, banks, trust companies, building and loan associations or land-mortgage or farm-loan associations) to secure annual licenses from a real estate commission hereby established (composed of three members—two appointed by the Commissioners and the assessor ex-officio). Applicants for licenses must be 21 years old, able to read and write English and must give proof of trustworthiness and competence in the business (not required if applicant has 2 years experience as a broker, etc., or in connection with real estate business in the District of Columbia). Bond required—$2,500 for brokers, $1,000 for salesmen—running to the District of Columbia; and a fee for broker’s license of $25 or $5 for salesman’s license. License may be revoked by the Commission, upon its own motion or on a verified complaint, after a hearing, for any fraudulent or dishonest dealing or conviction of a crime involving fraud or dishonesty. Revocation of a broker’s license suspends the license of every salesman under him.\nYou\u0026rsquo;ll notice that there are multiple dates specified. The date from the first line goes into the CSV file, but we want the latest action to go into the name of the text file for each summary. The function below handles that.\ndef format_latest_action_dates(summaries): actions = [] date_tags = [] intro_dates = [] for summary in summaries: # Break the summary text into lines lines = summary.splitlines() # Get the date on the first intro_dates.append(re.findall(DATE_PATTERN, lines[0])[0]) # If there is more than one line in the summary, store the second line, otherwise, store an empty string if len(lines) \u0026gt; 1: actions.append(lines[1]) else: actions.append(\u0026#34;\u0026#34;) # Process the second line of each summary for action in actions: # Get the whole date the bill was introduced intro_date = intro_dates[actions.index(action)] # Get the year from that date intro_year = intro_date[2] # Find all the dates in the second line of the summary dates = re.findall(DATE_PATTERN, action) # If there are dates in the second line, handle that if dates: # Sometimes no years are specified in the second line of the summary, as in the example above default_year = dates[0][2] or intro_year date = list(dates[-1]) if date[2] == \u0026#34;\u0026#34;: date[2] = default_year # Default to the year the bill was introduced else: date = list(intro_date) # Convert the month text to a two digit numerical string date[0] = MONTHS[date[0]] # Create the date string that will go into the name of the text file date_tag = f\u0026#34;{date[2].strip()}{date[0]}{date[1].zfill(2)}\u0026#34; date_tags.append(date_tag) return date_tags Output File Names # Here\u0026rsquo;s how we name the text file associated with each summary.\ndef format_output_files(text_file_names, date_tags): # If the number of text file base names and the number of date tags doesn\u0026#39;t match, we\u0026#39;ve got a problem if len(text_file_names) != len(date_tags): print(\u0026#34;Text files names list and date tags list are not the same length.\u0026#34;) sys.exit(1) # Zip the text file base names and date strings together, join them with \u0026#34;_\u0026#34;, and add the \u0026#34;.txt\u0026#34; extension filenames = map( lambda name: \u0026#34;_\u0026#34;.join(name) + \u0026#34;.txt\u0026#34;, list(zip(text_file_names, date_tags)) ) return list(filenames) Writing out the files # Here\u0026rsquo;s how we write the files to disk.\nWriting the CSV File # This is decently straightforward thanks to the csv module in the python standard library.\ndef write_metadata(metadata, output_file_name, congress): headers = [ \u0026#34;congress\u0026#34;, \u0026#34;session\u0026#34;, \u0026#34;pl_num\u0026#34;, \u0026#34;bill_type\u0026#34;, \u0026#34;bill_number\u0026#34;, \u0026#34;sponsor\u0026#34;, \u0026#34;committee\u0026#34;, \u0026#34;action_red\u0026#34;, \u0026#34;summary_version_code\u0026#34;, \u0026#34;report_number\u0026#34;, \u0026#34;action\u0026#34;, \u0026#34;action_date\u0026#34;, \u0026#34;associated_summary_file\u0026#34;, \u0026#34;questions_comments\u0026#34;, ] with open(output_file_name, \u0026#34;w\u0026#34;) as f: writer = csv.DictWriter(f, fieldnames=headers) writer.writeheader() for row in metadata: writer.writerow( { \u0026#34;congress\u0026#34;: congress, \u0026#34;session\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pl_num\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;bill_type\u0026#34;: row[0], \u0026#34;bill_number\u0026#34;: row[1], \u0026#34;sponsor\u0026#34;: row[2], \u0026#34;committee\u0026#34;: row[4] or \u0026#34;N/A\u0026#34;, \u0026#34;action_red\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;summary_version_code\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;report_number\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;Introduced\u0026#34;, \u0026#34;action_date\u0026#34;: row[3], \u0026#34;associated_summary_file\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;questions_comments\u0026#34;: \u0026#34;\u0026#34;, } ) Writing the Summary Files # This is the most straightforward function in the script. Essentially, we iterate over the summaries and match them up with the text file names we created.\ndef write_summaries(summaries, text_file_names): for i in range(len(summaries)): with open(text_file_names[i], \u0026#34;w\u0026#34;) as f: f.write(summaries[i]) Putting it all together # Now we actually do what we set out to do.\nif __name__ == \u0026#34;__main__\u0026#34;: args = parse_args() file_path = args.file_path start_page = args.start_page start_page_index = start_page - 1 end_page = args.end_page file_path = Path(file_path) file_stem = file_path.stem output_file_name = name_output_file(file_stem, start_page, end_page) congress = file_stem[:CONGRESS_POSITION] session = file_stem[SESSION_POSITION] metadata, summaries, text_file_names = extract_summaries_and_metadata( file_path, start_page_index, end_page ) action_dates = format_latest_action_dates(summaries) output_file_names = format_output_files(text_file_names, action_dates) write_metadata(metadata, output_file_name, congress) write_summaries(summaries, output_file_names) Up next # Now it\u0026rsquo;s time for the manual work. In the next post, I\u0026rsquo;ll detail how I use GNU Emacs to review each text file and Google Sheets to input additional metadata about each bill.\n"},{"id":3,"href":"/portfolio/docs/technology/metadata/2022-11-25-emacs-workflow/","title":"4. Emacs Workflow","section":"Metadata Workflow Series","content":" Emacs Workflow # We\u0026rsquo;ve run prep.sh (and with it, get_summaries.py), so now we\u0026rsquo;ve got a directory full of text files to review. This is where GNU Emacs comes in.\nInitially, I was doing all of this work on the command line, using iTerm2 on my Mac. I had a script that ran aspell on each text file before opening Neovim for each file for manual review. It worked fine, but it wasn\u0026rsquo;t very flexible, and to be honest, I\u0026rsquo;ve always wanted to become proficient in Emacs. This project presented me with a great opportunity to it a daily driver.\nIn the interest of keeping this post shorter than the last one, I\u0026rsquo;ll describe the workflow I landed on and not all the steps along the way.\nFinding the Directory # Within Emacs, I use C-x d to navigate to the directory created by prep.sh, e.g., 74_2_199. This opens Dired, the Emacs Directory Editor, where I next run dired-omit-mode with C-x M-o to hide uninteresting files (e.g., backup files that end with ~ and .bak).\nEditing the Files # Now, I use n and p to navigate between the listed files, hitting the enter key to open the file I\u0026rsquo;d like to edit. Within each file, I run M-x ispell with aspell as the backend for some automated spell-checking. I remove any weird entities introduced by OCR fix up line and paragraph breaks.\nThere are two really important elements of this step:\nEnter metadata about bill actions besides introduction into the spreadsheet Make sure the name of the file corresponds to the date of the latest action on the bill. Putting the data into Google Sheets is pretty straightforward, except for a couple of steps at the end of this element of the workflow that I\u0026rsquo;ll detail later in the post.\nIf I need to edit the name of the file, I do this back in the Dired buffer using the R key, making sure to change the name in the spreadsheet as well.\nFinalizing the spreadsheet data # Before wrapping up this element of the workflow, I do two things:\nI type the date into the spreadsheet as written in the original document, e.g., May 1, 1936, but I eventually reformat it into ISO 8601 using Format \u0026gt; Number in Google Sheets for some easier processing in SQLite later in the workflow. I fill down the Session column with the formula =if(year(L2088)=1935, 1, 2) (the L column is the Action Date column). Cleaning up and next steps # Finally within Emacs I run the script cleanup.sh, which will be explained in the next post, using the M-x ! key with the argument ../cleanup.sh.\n"},{"id":4,"href":"/portfolio/docs/technology/metadata/2022-11-25-cleanup/","title":"5. cleanup.sh","section":"Metadata Workflow Series","content":" cleanup.sh # With the text files edited, it\u0026rsquo;s time to clean up a little bit. That\u0026rsquo;s what cleanup.sh is for.\nAs usual, we start with the shebang line.\n#!/usr/bin/env zsh We remove all of the no longer necessary backups. The -f makes sure there are no complaints in case no backups exist.\n# remove temporary files rm -f *.bak rm -f *~ This section isn\u0026rsquo;t really necessary, but I added in to make working with one-off awk commands a little easier (tabs as delimiters are much easier to deal with than commas).\n# tsv file for easier command line text processing filepath=`realpath *.csv` dirname=`dirname $filepath` basename=`basename $filepath \u0026#39;.csv\u0026#39;` mlr --c2t cut -o -f bill_type,bill_number,sponsor,action_date,committee $filepath \u0026gt; \u0026#34;$dirname/$basename.tsv\u0026#34; echo \u0026#34;Created $dirname/$basename.tsv\u0026#34; Miller really shines here. awk could do the same thing, but with Miller I can use the names of the columns. In awk, I\u0026rsquo;d have to remember which numeric field variable, e.g., $1 corresponds to which column.\nHere, we add the names of all the text files in the directory to the clipboard for pasting into the spreadsheet.\nls *.txt | pbcopy echo \u0026#34;Filenames copied to clipboard.\u0026#34; Now it\u0026rsquo;s time to get rid of the backup directory created by prep.sh if it exists.\nPWD=$(pwd) if [[ -d \u0026#34;${PWD}.bak\u0026#34; ]] then rm -rf \u0026#34;${PWD}.bak\u0026#34; echo \u0026#34;Backup removed.\u0026#34; fi Finally, we move the directory up to its final location on my hard drive.\nmv $PWD $(dirname $(dirname $PWD)) echo \u0026#34;${PWD} moved.\u0026#34; cd .. Up next # Now it\u0026rsquo;s time for a little data munging with the script update-tsv-data.sh.\n"},{"id":5,"href":"/portfolio/docs/technology/metadata/2022-12-04-meta-data-sette/","title":"6. (meta)data(sette)","section":"Metadata Workflow Series","content":" (meta)data(sette) # Intro # After running cleanup.sh I like to make a local copy of it as a tab-separated file I can do various things with, including storing it in SQLite database with Datasette which, among other things, allows me to link the metadata with their associated text files.\nThe Code # As usual, it\u0026rsquo;s a shell script.\n#!/usr/bin/env zsh I use wget to save the data to my local machine. Getting the URL right took a little Googling and turned up this great answer on Stack Overflow. The spreadsheet has to be published to the web in order for this to work.\nwget -q -O ./all_data.tsv https://docs.google.com/spreadsheets/d/e/2PACX-1vQkoFBuqjcBX-a_xT3IfTmLmUJ0LZme8Mj16sIdqqEC9Z_vPmyHJKiTCRrZHPaSsxI-4lKaLkWD_XSk/pub\\?gid\\=0\\\u0026amp;single\\=true\\\u0026amp;output\\=tsv The Database # I then (re)create the SQLite database containing the bill metadata and the summary text files using Simon Willison\u0026rsquo;s extraordinarily easy to use sqlite-utils.\nFirst, I drop the existing tables that hold the core data.\nsqlite-utils drop-table summaries.db actions sqlite-utils drop-table summaries.db files Then I recreate the actions table, which is the table that holds the bill metadata\nsqlite-utils insert summaries.db actions all_data.tsv --tsv -d I rename the columns for from the spreadsheet for use in the database. There may be a better way to do this, but this works.\nsqlite-utils transform summaries.db actions --rename \u0026#34;Congress\u0026#34; \u0026#34;congress\u0026#34; --rename \u0026#34;Session\u0026#34; \u0026#34;session\u0026#34; --rename \u0026#34;PL Num (if applicable)/PVTL Num\u0026#34; \u0026#34;pl_num\u0026#34; --rename \u0026#34;Bill Type\u0026#34; \u0026#34;bill_type\u0026#34; --rename \u0026#34;Bill Number \u0026#34; \u0026#34;bill_number\u0026#34; --rename \u0026#34;Sponsor\u0026#34; \u0026#34;sponsor\u0026#34; --rename \u0026#34;Committee\u0026#34; \u0026#34;committee\u0026#34; --drop \u0026#34;Action Code\u0026#34; --drop \u0026#34;Summary Version Code\u0026#34; --drop \u0026#34;Report Number\u0026#34; --rename \u0026#34;Action\u0026#34; \u0026#34;action\u0026#34; --rename \u0026#34;Action Date\u0026#34; \u0026#34;action_date\u0026#34; --rename \u0026#34;Associated Summary Text File Name\u0026#34; \u0026#34;summary_text_file_name\u0026#34; --rename \u0026#34;Questions/Comments\u0026#34; \u0026#34;questions_comments\u0026#34; Now I insert all the text file summaries on my local machine into the database.\nsqlite-utils insert-files --text summaries.db files 74*/*.txt This ends up saving the directory name and file name in the files tables\u0026rsquo;s path column. We just want the file name since that\u0026rsquo;s what is recorded in the actions table. The following fixes that for us.\nsqlite-utils convert summaries.db files path \u0026#39;value.split(\u0026#34;/\u0026#34;)[1]\u0026#39; Now we can link up the two tables\nsqlite-utils add-foreign-key summaries.db actions summary_text_file_name files path --ignore and publish the database as an app on Heroku.\ndatasette publish heroku summaries.db --name \u0026#34;llc\u0026#34; --tar \u0026#34;/usr/local/bin/gtar\u0026#34; --install=datasette-saved-queries Local Copies of the Data # Next I massage the data a little bit to allow for some simple analysis on the command line with MIller. The awk command drops the columns that don\u0026rsquo;t have any data in them and the Miller command fills empty spots in the Sponsor and Committee columns, making the data bit tidy-er.\nawk \u0026#39;BEGIN{FS=OFS=\u0026#34;\\t\u0026#34;}{print $1, $2, $4, $5, $6, $7, $11, $12}\u0026#39; all_data.tsv | mlr --tsv fill-down -f Sponsor,Committee then cat \u0026gt; filled_metadata.tsv Finally, to allow for easier reading of the spreadsheet by humans, the following awk command puts a blank line between each bill. $5 refers to the Bill Number column.\n# insert blank lines between bills for pasting into final spreadsheet awk \u0026#39;BEGIN{FS=\u0026#34;\\t\u0026#34;} {cur=$5} NR\u0026gt;1 \u0026amp;\u0026amp; cur!=prev {print \u0026#34;\u0026#34;} {prev=cur; print}\u0026#39; all_data.tsv \u0026gt; spaced_metadata.tsv Next Time # This wraps up the core workflow, so now it\u0026rsquo;s time to move on how I actually use the Datasette web interface and SQL queries to check metadata quality.\n"},{"id":6,"href":"/portfolio/docs/outcomes/articulate/","title":"Critically Articulating the Philosophy, Principles, and Ethics of Library and Information Science","section":"Program Learning Outcomes and Work Samples","content":" Critically articulating the philosophy, principles, and ethics of library and information science # LS 501: Scholarly Communication Reading Response LS 501: Final Essays "},{"id":7,"href":"/portfolio/docs/outcomes/evaluate/","title":"Evaluating Technology-Mediated Access in Library and Information Services","section":"Program Learning Outcomes and Work Samples","content":" Evaluating technology-mediated access in library and information services # LS 562: Digital Collection Development Policy Analysis LS 566: Metadata Quality Problems "},{"id":8,"href":"/portfolio/docs/philosophy/","title":"Philosophy of LIS Practice","section":"Docs","content":" A Reflective Essay on Information, Technology, Humanity, and Purpose # I distinctly remember what first piqued my interest in computing and technology. I was in sixth or seventh grade, spending the night at a friend\u0026rsquo;s house, and before settling in for a late night and early morning of action movie after action movie followed by far too little sleep, we sat at the desktop computer in the basement guestroom. Instead of playing through one of his favorite science fiction flight simulator games, my friend showed me how some text surrounded by angle brackets he typed out in NotePad could turn into a website anyone anywhere in the world could visit thanks to AngelFire. HTML: HyperText Markup Language. Language. I knew language. I loved language! French and English were my two favorite classes in school at the time. All it took to get a computer to do what you wanted was to learn its language? I could do that. I was hooked.\nIn the almost 25 years since that night, that interest in the intersection of language and technology has gone through ups and downs, wavering but never flickering out. I shudder to think how much money I\u0026rsquo;ve spent on programming language books and courses that I never finished \u0026ndash; Java, PHP, Ruby, Python, Perl, C, Haskell, OCaml, Standard ML, Scheme, Racket, Common Lisp, JavaScript, Clojure, Elm, Go, Smalltalk, Prolog, R, and others I\u0026rsquo;m sure I\u0026rsquo;m forgetting. Once I realized how much math figured into this field, something I unfortunately decided I wasn\u0026rsquo;t good at a young age because I wasn\u0026rsquo;t as fast at it as my peers (and in which I tried to justify or feign disinterest as I announced that it wasn\u0026rsquo;t necessary for what I wanted to do), I more or less gave up, again and again, immersing myself in and identifying with the more human side of my interest in language, ultimately earning a bachelor\u0026rsquo;s degree in classical languages with an emphasis on Ancient Greek.\nI think what ties these scattershot interests together is an interest in structure, form, and, most importantly, meaning. What motivates me now, I think, is learning how to use these interests to help people, which has been the focus of my career up to this point. Each job I have held since college, from working as a community organizer in inner-city Birmingham, serving as a youth minister at an Episcopal church in Huntsville, and now teaching Latin at a private school in suburban Birmingham, has involved navigating, organizing, and interpreting information. In each position, there have been situations in which I have asked or been asked to answer questions that should be easy to answer, but haven\u0026rsquo;t been because of a lack of underlying form, structure, or organization to the information available to help answer the question. In other words, a lack of meaning, or semantic context. So my goal is this: to develop interpersonal and technical skills that will help human-serving organizations confidently manage the information at their disposal.\nMy understanding of information is becoming grounded in Buckland\u0026rsquo;s conceptualization of information as thing, because as Buckland says, \u0026ldquo;expert systems and information retrieval systems\u0026rdquo; \u0026ndash; in other words, technological systems for working with information \u0026ndash; \u0026ldquo;can only deal with information in the sense of information as thing\u0026rdquo; (352). Human-serving organizations abound with instances of information as thing: policy documents, meeting minutes, emails, blog posts, newsletters, bills, receipts, spreadsheets, invoices, and more. I believe that technology has a role to play in managing these informative objects but that in many cases, the necessary skills are lacking from these organizations. I want to be someone who can bring such skills to the table for these human-serving organizations, all while being careful to avoid implementing technology for technology\u0026rsquo;s sake and instead recalling Ma\u0026rsquo;s dictum that\nThe design of information systems and the education of information professionals depend upon a sound theory of communication, one that describes and explains how humans really interact with each other and how meanings are constructed through acts of understanding. (718-719)\nIn other words: the answer to the question \u0026ldquo;What is the purpose of all of this?\u0026rdquo; is understanding and collaborative meaning construction.\nAs an aspiring information professional and technologist, I hope to carry the message of Zobel\u0026rsquo;s work on information retrieval (IR) into my practice. What so impressed me about the paper of his that we read this semester is that purpose is at the center of his conceptualization of IR. I found the following definition of IR from his paper inspiring: \u0026ldquo;the study of techniques for supporting human cognition with documents, using material that is sourced from large document collections\u0026rdquo; (25).\n\u0026ldquo;Supporting human cognition.\u0026rdquo; Not replacing, but supporting. In other words, being mindful of who and what this work is for. Does artificial intelligence have a role to play in any of this? If so, what do we mean by artificial intelligence, and what is that role?\nOne place to start as we attempt to answer these questions is with the work of Alan Turing, the mathematician and computer scientist whom we have to thank (or perhaps from some perspectives, to blame) for computing as we know it today. This essay is not an appropriate venue for a thorough discussion of Turing\u0026rsquo;s work, nor am I anywhere near expert enough in it to make any claims except for this: that we cannot separate the history of computing from the history of artificial intelligence. For more on this idea, Sebastian Sunday Grève\u0026rsquo;s recent essay for Aeon is a good place to start.\nWe should also acknowledge that there is an undeniably darker, and in some cases sinister, side to artificial intelligence as well. Introducing a series of articles on \u0026ldquo;AI colonialism\u0026rdquo; for the MIT Technology Review, Karen Hao writes \u0026ldquo;The more users a company can acquire for its products, the more subjects it can have for its algorithms, and the more resources—data—it can harvest from their activities, their movements, and even their bodies.\u0026rdquo; To what end? Writing for Politico\u0026rsquo;s Digital Future Daily newsletter, Derek Robertson synthesizes a Twitter thread in which Emily Bender, a computational linguist at the University of Washington, criticizes Eric Jang\u0026rsquo;s essay in which he lays out his \u0026ldquo;ambitions to build an artificial general intelligence that would encapsulate the entirety of human experience.\u0026rdquo; Robertson says the thread criticizing the essay is\na useful case study in two different technological visions of the world: One where progress is inevitable, competition is zero-sum, and engineers should be single-mindedly focused on getting there first, and one where technological advancement is more holistic and human-centered.\nJust how far does the former of these two visions go? In an essay for Current Affairs, Phil Torres investigates a philosophy called longtermism, \u0026ldquo;the idea that what matters most is for “Earth-originating intelligent life” to fulfill its potential in the cosmos.\u0026rdquo; Though it may seem initially unrelated to AI, Torres says that \u0026ldquo;many longtermists believe that superintelligent machines pose the greatest single hazard to human survival, but they seem convinced that if humanity were to create a “friendly” superintelligence whose goals are properly “aligned” with our “human goals,” then a new Utopian age of unprecedented security and flourishing would suddenly commence.\u0026rdquo;\nThe latter, and much less extreme vision, is illustrated on the about page of Timnit Gebru\u0026rsquo;s DAIR, the Distributed AI Research Institute, \u0026ldquo;an interdisciplinary and globally distributed AI research institute rooted in the belief that AI is not inevitable, its harms are preventable, and when its production and deployment include diverse perspectives and deliberate processes it can be beneficial.\u0026rdquo;\nThis is the vision we should strive for, the vision that is in line with Ma\u0026rsquo;s and Zobel\u0026rsquo;s views, and the vision that I would want to be a part of. Information is for people, all people, and technology that does not recognize and advance that point of view can be harmful and dangerous. I am grateful to be taking my first steps into LIS, a field which, though technologically informed, places people\u0026rsquo;s needs first.\nReferences:\nBuckland, M. K. (1991). Information as thing. Journal of the American Society for Information Science, 42 (5), 351-360.\nDAIR. \u0026ldquo;About.\u0026rdquo; https://www.dair-institute.org/about. Accessed May 1, 2022.\nGrève, Sebastian S. \u0026ldquo;AI’s first philosopher.\u0026rdquo; https://aeon.co/essays/why-we-should-remember-alan-turing-as-a-philosopher. Accessed May 1, 2022.\nHao, Karen. \u0026ldquo;Artificial intelligence is creating a new colonial world order.\u0026rdquo; https://www.technologyreview.com/2022/04/19/1049592/artificial-intelligence-colonialism/. Accessed May 1, 2022.\nMa, L. (2012). Meanings of information: The assumptions and research consequences of three foundational LIS theories. Journal of the American Society for Information Science, 63 (4), 716-723.\nRobertson, Derek. https://www.politico.com/newsletters/digital-future-daily/2022/04/29/cryptos-strange-new-respectability-00029062. Accessed May 1, 2022.\nZobel, J., (2017). What we talk about when we talk about information retrieval. ACM SIGIR Forum, 51 (3), 18-26.\n"},{"id":9,"href":"/portfolio/docs/outcomes/practice/","title":"Practicing Principles of Social and Cultural Justice","section":"Program Learning Outcomes and Work Samples","content":" Practicing principles of social and cultural justice in my preparation for a career in library and information environments # LS 501: Critical Cataloging LS 506: Ethical Cataloging "},{"id":10,"href":"/portfolio/docs/outcomes/","title":"Program Learning Outcomes and Work Samples","section":"Docs","content":" Program Learning Outcomes and Work Samples # Students will: # evaluate technology-mediated access in library and information services # use evidence to inform library and information practices # critically articulate the philosophy, principles, and ethics of library and information science # practice principles of social and cultural justice in their preparation for careers in library and information environments # "},{"id":11,"href":"/portfolio/docs/resume/","title":"Resume","section":"Docs","content":" William T. Blackerby # 332 Willow Bend Road\nBirmingham, AL 35209\n205-240-3705\nwmblackerby@gmail.com • LinkedIn • Github • Blog\nEducation # The University of Alabama, Tuscaloosa, AL\nMaster of Library and Information Studies (online)\nexpected completion Spring 2023\nCompleted coursework: LS 500: Information Science and Technology, LS 501: Information and Communities, LS 535: Records Management, LS 566: Metadata and Semantic Web Fundamentals, LS 590: Linked Data, LS 506: Modern Cataloging and Classification, LS 562: Digital Libraries, LS 564: Programming for Digital Libraries, LS 570: Internship (Law Library of Congress Remote Metadata Internship) Coursework in progress Spring 2023: LS 513: Professional Paths, LS 569: Information Management, LS 570: Internship (Southern Music Research Center) The University of the South, Sewanee, TN\nBachelor of Arts, cum laude, 2009\nMajor: Classical Languages Omicron Delta Kappa (national leadership honor society) Order of the Gown (academic honor society), 2007-2009 Experience # Law Library of Congress Remote Metadata Intern, August 2022-December 2022\nPart of a team assigned to extract text and metadata from Congressional Research Service bill digests for the 74th-76th Congresses Developed several Python and shell scripts to automate aspects of the workflow Indian Springs School\nLatin and Greek Teacher, August 2016-Present\nTeach Latin I, II, III, IV, AP Latin and Introduction to Ancient Greek Modern and Classical Languages Department Chair, July 2021-Present\nSupervise five World Languages teachers Manage departmental budget Dean of Students, July 2019-June 2021\nMember of Administrative Leadership Team Managed Student Activities budget Managed student records including vehicle registrations, permission forms, discipline letters Oversaw student government activities, including elections twice per year Greater Birmingham Ministries, Birmingham, AL\nFaith in Community Organizer, July 2009 – January 2013\nHired, trained, and supervised six part-time staff for 2012 non-partisan voter engagement campaign Maintained organization’s website, blog, and Facebook and Twitter accounts Coordinated production of quarterly newsletter, including writing and editing articles Represented organization at public events, including congregational mission/outreach fairs Technologies # Programming languages: Ruby (intermediate), Python (intermediate), R (intermediate), PHP (intermediate) Web technologies: HTML (intermediate), Markdown (intermediate) Data technologies: Google Sheets (intermediate), Microsoft Excel (intermediate), SPARQL (intermediate), OpenRefine (intermediate), MySQL (intermediate), SQLite (intermediate) Tools: Unix command line (intermediate), Git version control system (intermediate), bash/zsh shell scripting (intermediate), awk (intermediate), sed (intermediate), miller (intermediate), emacs (intermediate), vim (beginning) Languages # Latin (Full professional proficiency) French (Limited working profiency) Ancient Greek (Limited working proficiency) Community Involvement # Alabama Arise/Alabama Arise Action\nBoard of Directors, 2014-2020\nStatewide organization advocating for state policies to improve the lives of low-income Alabamians Served one term as President of the Board of Directors Activities included: chairing board meetings and conference calls; emceeing special events; consulting with executive director on finance, development, programs, hiring, and special projects, e.g., organizational commitment to racial equity and inclusion; committee service including finance, development, nominations, and personnel; conducting executive director performance review. Gasp, Inc.\nBoard of Directors, 2011-2021\nMembership-based organization advancing healthy air and environmental justice in the greater Birmingham area through education, advocacy, and collaboration Multiple terms as Secretary of the Board of Directors Activities included: taking minutes at every board meeting, serving on development committee, participating in executive director performance review. Awards and Honors # Eagle Scout, 2005 Second Place, Koine Greek, Eta Sigma Phi Maurine Dallas Watkins Sight Translation Contest, 2007 Isaac Marion Dwight Medal for Philosophical Greek, University of the South, 2009 "},{"id":12,"href":"/portfolio/docs/outcomes/use/","title":"Using Evidence to Inform Library and Information Practices","section":"Program Learning Outcomes and Work Samples","content":" Using evidence to inform library and information practices # LS 506: Metadata Quality LS 535 Memo "},{"id":13,"href":"/portfolio/docs/technology/2022-12-02-senate-topic-modeling/","title":"Visualizing Topics of 74th Congress Senate Bills and Joint Resolutions","section":"Technology","content":" Visualizing Topics of 74th Congress Senate Bills and Joint Resolutions # This code is available to experiment with on Binder in a notebook with the filename tmr.ipynb. Be sure to check out the ner_mapping.ipynb notebook as well, in which I create a heatmap of states mentioned in Senate bills and joint resolutions during the 74th Congress.\nIn LS 562: Digital Libraries, we studied the application of machine learning tools to text analysis problems. For practice, I decided to apply one of those tools, topic modeling, to the corpus of 74th Congress Senate bills and join resolutions I processed during my remote metadata internship with the Law Library of Congress.\nSetting Up # The Necessary Libraries # Below, we load the libraries this project uses into my work environment. tidyverse, tidytext, and ggplot2 are pretty well know inhabitants of the R Tidyverse, so I won\u0026rsquo;t explain them here. stm is the package used in this project for topic modeling. LDAvis is the tool we will use to visualize the LDA model applied to Senate bill corpus. quanteda is a text analysis library, and spacyr is an R wrapper for the Python natural language processing library spaCy. gistr is necessary for publishing the visualization to the web.\nlibrary(tidyverse) library(tidytext) library(ggplot2) library(stm) library(LDAvis) library(quanteda) library(spacyr) library(gistr) Initializing spaCy # Below, we initialize spaCy with its built-in English language model.\nspacy_initialize(model = \u0026#34;en_core_web_sm\u0026#34;) Load dataset # The data we will use is available for download as a CSV file from a simple API created using Datasette. I have crafted a SQL query to get just the data that we need. This query will return a table containing each bill summary\u0026rsquo;s unique ID in the database and its text.\ndata \u0026lt;- read.csv(\u0026#34;https://llc.herokuapp.com/summaries.csv?sql=select%0D%0A++rowid%2C%0D%0A++content_text%0D%0Afrom%0D%0A++files%0D%0Awhere%0D%0A++%22path%22+like+%2274_2_s%25%22%0D%0Aorder+by%0D%0A++path\u0026amp;_size=max\u0026#34;) head(data) Preprocessing # With the environment set up and the data acquired, we can start massaging the data into the shape the topic modeling algorithm needs.\nspaCy # First, we put the text of all the bills we downloaded into one big string which we then use spaCy to parse.\ntext \u0026lt;- data$content_text %\u0026gt;% str_c() parsed_text \u0026lt;- spacy_parse(text, lemma = TRUE, entity = TRUE, nounphrase = TRUE) head(parsed_text) Getting the words we want # Next, we reduce the corpus to tokens that have been part-of-speech tagged as proper nouns, verbs, nouns, adjectives, and adverbs. The part-of-speech tagging isn\u0026rsquo;t perfect, so we remove tokens tagged as those parts of speech that we know aren\u0026rsquo;t, actually. Finally, make all of the lemmas lowercase and create the word column.\ntokens \u0026lt;- parsed_text %\u0026gt;% filter(pos == \u0026#34;PROPN\u0026#34; | pos == \u0026#34;VERB\u0026#34; | pos == \u0026#34;NOUN\u0026#34; | pos == \u0026#34;ADJ\u0026#34; | pos == \u0026#34;ADV\u0026#34;) %\u0026gt;% filter(lemma != \u0026#34;.\u0026#34; \u0026amp; lemma != \u0026#34;Mr.\u0026#34; \u0026amp; str_detect(lemma, \u0026#34;^\\\\w\\\\.?$\u0026#34;, negate = TRUE)) %\u0026gt;% filter(lemma != \u0026#34;§\u0026#34;) %\u0026gt;% mutate(word = tolower(lemma)) Next we get rid of stop words\ndata(\u0026#34;stop_words\u0026#34;) tokens \u0026lt;- tokens %\u0026gt;% anti_join(stop_words) And now we can take a quick look at the tokens that appear more than 100 times in the bill summaries.\ntokens %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 100) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) The names of months show up a lot, but they won\u0026rsquo;t contribute meaningfully to the topic model. Let\u0026rsquo;s get rid of them\u0026hellip;\nmonth_tokens \u0026lt;- tibble(month.name) %\u0026gt;% unnest_tokens(word, month.name) tokens \u0026lt;- tokens %\u0026gt;% anti_join(month_tokens) \u0026hellip;and see what our chart looks like now\ntokens %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 100) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) This is better, but there\u0026rsquo;s more we can do. First, let\u0026rsquo;s remove some more words that might clutter our model.\nThese calls bring the text of bill types and the names of sponsors identified in the original document into the environment.\nbill_types \u0026lt;- read.csv(\u0026#34;https://llc.herokuapp.com/summaries.csv?sql=select+distinct+bill_type+from+actions\u0026amp;_size=max\u0026#34;) sponsors \u0026lt;- read.csv(\u0026#34;https://llc.herokuapp.com/summaries.csv?sql=select+distinct+sponsor+from+actions+where+sponsor+is+not+null+and+sponsor+%21%3D+%22%22\u0026amp;_size=max\u0026#34;) We can now make that text tokens\nbill_type_tokens \u0026lt;- bill_types %\u0026gt;% unnest_tokens(word, bill_type) sponsor_tokens \u0026lt;- sponsors %\u0026gt;% unnest_tokens(word, sponsor) and remove them from our corpus.\ntokens \u0026lt;- tokens %\u0026gt;% anti_join(bill_type_tokens) %\u0026gt;% anti_join(sponsor_tokens) How does our chart look now?\ntokens %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% filter(n \u0026gt; 100) %\u0026gt;% mutate(word = reorder(word, n)) %\u0026gt;% ggplot(aes(n, word)) + geom_col() + labs(y = NULL) It doesn\u0026rsquo;t look like any change, but we have reduced the number of tokens in our corpus. I think we\u0026rsquo;re in good enough shape to do our topic modeling.\nTopic Modeling # A document feature matrix # The stm() function exposed by the stm package can\u0026rsquo;t take our data frame as input. Instead, it needs a document-term matrix, in this case in the form of a document-feature matrix, which we create with the tidytext cast_dfm() function.\ndfm \u0026lt;- tokens %\u0026gt;% count(doc_id, word, sort = TRUE) %\u0026gt;% cast_dfm(doc_id, word, n) We\u0026rsquo;re ready to apply the model! I\u0026rsquo;ve set k, the number of topics, as 48 because there were 48 Senate committees in 74th Congress. The following line of code takes a few minutes to finish executing.\nmodel \u0026lt;- stm(dfm, K = 48, init.type = \u0026#34;LDA\u0026#34;, seed = 1234, verbose = FALSE) Visualization # Before we can visualize our model with LDAvis, we need to put our document-term matrix into a form that toLDAvis, an stm-provided wrapper around LDAvis, can understand. We do that with the call to convert() below. The call to toLDAvis creates the visualization as a static website and publishes it online.\nlda \u0026lt;- convert(x = dfm, to = \u0026#34;lda\u0026#34;) ldavis \u0026lt;- toLDAvis(model, lda$documents, R = 48, open.browser = interactive(), as.gist = TRUE) You can see the final product here.\nWrapping up # Poking around in the visualization a little bit, it becomes clear that there\u0026rsquo;s some more refinement of our data we could do, e.g., removing any token with a numeral in it, removing more honorifics than just \u0026ldquo;Mr.\u0026rdquo;, and perhaps removing the names of actions like \u0026ldquo;approve\u0026rdquo;, \u0026ldquo;pass\u0026rdquo;, and \u0026ldquo;refer\u0026rdquo;. That said, I think that this visualization does offer a good sense of the topics under discussion in various Senate committees in the 74th, and creating it was a great skill-building exercise.\n"}]